#!/usr/bin/env python3
"""
Full Control HPO - å®Œå…¨åƒæ•¸åŒ–çš„MAT-HPOå„ªåŒ–å™¨
ä¿ç•™æ‰€æœ‰æ ¸å¿ƒåŠŸèƒ½ï¼Œå…è¨±ç”¨æˆ¶æ§åˆ¶æ¯å€‹åƒæ•¸
"""

from typing import Dict, Any, Optional
from pathlib import Path

from .core.llm_enhanced_optimizer import LLMEnhancedMAT_HPO_Optimizer, LLMEnhancedOptimizationConfig
from .core.enhanced_environment import TimeSeriesEnvironment
from .core.hyperparameter_space import HyperparameterSpace
from .utils.llm_config import create_llm_config


class FullControlHPO:
    """
    å®Œå…¨æ§åˆ¶çš„HPOæ¥å£ - æš´éœ²æ‰€æœ‰MAT-HPO + LLMåƒæ•¸

    é€™å€‹é¡ä¸éš±è—ä»»ä½•åƒæ•¸ï¼Œè®“ç”¨æˆ¶å®Œå…¨æ§åˆ¶å„ªåŒ–æµç¨‹
    """

    def __init__(self,
                 # åŸºæœ¬é…ç½®
                 task_type: str = "time_series_classification",
                 max_trials: int = 30,

                 # LLM é…ç½®
                 llm_enabled: bool = True,
                 llm_model: str = "llama3.2:3b",
                 llm_base_url: str = "http://localhost:11434",
                 mixing_strategy: str = "fixed_alpha",

                 # LLM æ··åˆåƒæ•¸ - å®Œå…¨æš´éœ²
                 alpha: float = 0.3,                    # å›ºå®šæ··åˆæ¯”ä¾‹
                 slope_threshold: float = 0.01,         # ç·šæ€§å›æ­¸æ–œç‡é–¾å€¼
                 llm_cooldown_episodes: int = 5,        # LLM å†·å»æœŸ
                 min_episodes_before_llm: int = 5,      # æœ€å°episodeå¾Œæ‰èƒ½ç”¨LLM

                 # RL é…ç½® - å®Œå…¨æš´éœ²
                 replay_buffer_size: int = 1000,        # ç¶“é©—å›æ”¾ç·©è¡å€å¤§å°
                 gamma: float = 0.99,                   # æŠ˜æ‰£å› å­
                 tau: float = 0.001,                    # è»Ÿæ›´æ–°ä¿‚æ•¸
                 batch_size: int = 64,                  # RLæ‰¹æ¬¡å¤§å°
                 learning_rate: float = 0.001,          # RLå­¸ç¿’ç‡
                 actor_lr: float = 0.001,              # Actorå­¸ç¿’ç‡
                 critic_lr: float = 0.002,             # Criticå­¸ç¿’ç‡
                 hidden_dim: int = 256,                # éš±è—å±¤ç¶­åº¦

                 # å…¶ä»–é«˜ç´šåƒæ•¸
                 seed: int = 42,                       # éš¨æ©Ÿç¨®å­
                 device: str = "auto",                 # è¨­å‚™é¸æ“‡
                 verbose: bool = True,
                 output_dir: str = "./full_control_results",

                 # æ•¸æ“šé›†åƒæ•¸
                 dataset_info_csv_path: Optional[str] = None,
                 custom_prompt_template: Optional[str] = None,
                 dataset_name: Optional[str] = None,
                 task_description: Optional[str] = None,

                 **kwargs):
        """
        åˆå§‹åŒ–å®Œå…¨æ§åˆ¶HPOå„ªåŒ–å™¨

        Args:
            task_type: ä»»å‹™é¡å‹
            max_trials: æœ€å¤§è©¦é©—æ¬¡æ•¸

            # LLM åƒæ•¸
            llm_enabled: æ˜¯å¦å•Ÿç”¨LLM
            llm_model: LLMæ¨¡å‹åç¨±
            llm_base_url: LLMæœå‹™URL
            mixing_strategy: æ··åˆç­–ç•¥ ("fixed_alpha", "adaptive", "llmpipe", "hybrid")
            alpha: å›ºå®šæ··åˆæ¯”ä¾‹ (0-1)
            slope_threshold: æ€§èƒ½æ–œç‡é–¾å€¼ï¼Œä½æ–¼æ­¤å€¼è§¸ç™¼LLM
            llm_cooldown_episodes: LLMèª¿ç”¨å¾Œçš„å†·å»æœŸ
            min_episodes_before_llm: ä½¿ç”¨LLMå‰çš„æœ€å°episodeæ•¸

            # RL åƒæ•¸
            replay_buffer_size: ç¶“é©—å›æ”¾ç·©è¡å€å¤§å°
            gamma: æŠ˜æ‰£å› å­
            tau: è»Ÿæ›´æ–°ä¿‚æ•¸
            batch_size: RLæ‰¹æ¬¡å¤§å°
            learning_rate: RLå­¸ç¿’ç‡
            actor_lr: Actorç¶²çµ¡å­¸ç¿’ç‡
            critic_lr: Criticç¶²çµ¡å­¸ç¿’ç‡
            hidden_dim: ç¶²çµ¡éš±è—å±¤ç¶­åº¦

            # å…¶ä»–åƒæ•¸
            seed: éš¨æ©Ÿç¨®å­
            device: è¨ˆç®—è¨­å‚™
            verbose: æ˜¯å¦é¡¯ç¤ºè©³ç´°ä¿¡æ¯
            output_dir: è¼¸å‡ºç›®éŒ„
        """

        # å­˜å„²æ‰€æœ‰åƒæ•¸
        self.task_type = task_type
        self.max_trials = max_trials

        # LLM åƒæ•¸
        self.llm_enabled = llm_enabled
        self.llm_model = llm_model
        self.llm_base_url = llm_base_url
        self.mixing_strategy = mixing_strategy
        self.alpha = alpha
        self.slope_threshold = slope_threshold
        self.llm_cooldown_episodes = llm_cooldown_episodes
        self.min_episodes_before_llm = min_episodes_before_llm

        # RL åƒæ•¸
        self.replay_buffer_size = replay_buffer_size
        self.gamma = gamma
        self.tau = tau
        self.batch_size = batch_size
        self.learning_rate = learning_rate
        self.actor_lr = actor_lr
        self.critic_lr = critic_lr
        self.hidden_dim = hidden_dim

        # å…¶ä»–åƒæ•¸
        self.seed = seed
        self.device = device
        self.verbose = verbose
        self.output_dir = Path(output_dir)

        # æ•¸æ“šé›†åƒæ•¸
        self.dataset_info_csv_path = dataset_info_csv_path
        self.custom_prompt_template = custom_prompt_template
        self.dataset_name = dataset_name or f"FullControl_{task_type}"
        self.task_description = task_description

        # ç¢ºä¿è¼¸å‡ºç›®éŒ„å­˜åœ¨
        self.output_dir.mkdir(parents=True, exist_ok=True)

        # åˆå§‹åŒ–çµ„ä»¶
        self.environment = None
        self.hyperparameter_space = None
        self.optimizer = None
        self.results = None

        if verbose:
            print(f"ğŸ›ï¸  Full Control HPO initialized:")
            print(f"   Task: {task_type}")
            print(f"   Max trials: {max_trials}")
            print(f"   LLM: {'âœ… Enabled' if llm_enabled else 'âŒ Disabled'}")
            if llm_enabled:
                print(f"   LLM Strategy: {mixing_strategy}")
                print(f"   Alpha: {alpha}")
                print(f"   Slope threshold: {slope_threshold}")
                print(f"   Cooldown episodes: {llm_cooldown_episodes}")
            print(f"   RL Buffer size: {replay_buffer_size}")
            print(f"   RL Learning rate: {learning_rate}")
            print(f"   Output: {output_dir}")

    def optimize(self,
                 X_train, y_train,
                 X_val=None, y_val=None,
                 X_test=None, y_test=None,
                 custom_space: Optional[HyperparameterSpace] = None,
                 custom_environment = None) -> Dict[str, Any]:
        """
        é‹è¡Œå®Œå…¨æ§åˆ¶çš„å„ªåŒ–

        Args:
            X_train, y_train: è¨“ç·´æ•¸æ“š
            X_val, y_val: é©—è­‰æ•¸æ“š
            X_test, y_test: æ¸¬è©¦æ•¸æ“š
            custom_space: è‡ªå®šç¾©è¶…åƒæ•¸ç©ºé–“
            custom_environment: è‡ªå®šç¾©ç’°å¢ƒ

        Returns:
            å„ªåŒ–çµæœå­—å…¸
        """

        # è‡ªå‹•åˆ†å‰²é©—è­‰é›†
        if X_val is None and y_val is None:
            from sklearn.model_selection import train_test_split
            import numpy as np
            X_train, X_val, y_train, y_val = train_test_split(
                X_train, y_train, test_size=0.2, random_state=self.seed,
                stratify=y_train if len(np.unique(y_train)) > 1 else None
            )
            if self.verbose:
                print(f"ğŸ“Š Auto-split data: train={len(X_train)}, val={len(X_val)}")

        # è¨­ç½®ç’°å¢ƒ
        if custom_environment:
            self.environment = custom_environment
        else:
            self.environment = TimeSeriesEnvironment(
                task_type=self.task_type,
                dataset_name=self.dataset_name
            )
            self.environment.set_data(X_train, y_train, X_val, y_val, X_test, y_test)
            self.environment.load_data()

        # è¨­ç½®è¶…åƒæ•¸ç©ºé–“
        if custom_space:
            self.hyperparameter_space = custom_space
        else:
            self.hyperparameter_space = self._create_default_space()

        # å‰µå»ºå®Œå…¨é…ç½®çš„LLMé…ç½®
        llm_config = LLMEnhancedOptimizationConfig(
            # åŸºæœ¬RLåƒæ•¸ - åªå‚³é OptimizationConfig æ”¯æŒçš„åƒæ•¸
            max_steps=self.max_trials,
            replay_buffer_size=self.replay_buffer_size,
            batch_size=self.batch_size,
            policy_learning_rate=self.actor_lr,
            value_learning_rate=self.critic_lr,
            
            # è¨­å‚™å’Œç¨®å­åƒæ•¸
            seed=self.seed,
            gpu_device=0,  # ä½¿ç”¨é»˜èªGPUè¨­å‚™
            use_cuda=self.device != "cpu",
            verbose=self.verbose,

            # LLMåƒæ•¸
            enable_llm=self.llm_enabled,
            llm_model=self.llm_model,
            llm_base_url=self.llm_base_url,
            mixing_strategy=self.mixing_strategy,
            alpha=self.alpha,
            slope_threshold=self.slope_threshold,
            llm_cooldown_episodes=self.llm_cooldown_episodes,
            min_episodes_before_llm=self.min_episodes_before_llm,

            # æ•¸æ“šé›†å’Œæç¤ºåƒæ•¸
            dataset_name=self.dataset_name,
            task_description=self.task_description or self.environment.get_llm_context(),
            custom_prompt_template=self.custom_prompt_template,
            dataset_info_csv_path=self.dataset_info_csv_path
        )

        # å‰µå»ºå„ªåŒ–å™¨
        self.optimizer = LLMEnhancedMAT_HPO_Optimizer(
            environment=self.environment,
            hyperparameter_space=self.hyperparameter_space,
            config=llm_config,
            output_dir=str(self.output_dir)  # Pass custom output directory
        )

        if self.verbose:
            print(f"\nâš¡ Starting full control optimization ({self.max_trials} trials)...")
            if self.llm_enabled:
                print(f"   ğŸ¤– LLM Strategy: {self.mixing_strategy}")
                print(f"   ğŸ¯ Alpha: {self.alpha}, Slope threshold: {self.slope_threshold}")
            print(f"   ğŸ§  RL Buffer: {self.replay_buffer_size}, LR: {self.learning_rate}")

        # é‹è¡Œå„ªåŒ–
        import time
        start_time = time.time()
        self.results = self.optimizer.optimize()

        # æ·»åŠ è¨ˆæ™‚ä¿¡æ¯
        end_time = time.time()
        self.results['optimization_time'] = end_time - start_time
        self.results['trials_completed'] = self.max_trials

        if self.verbose:
            print(f"\nğŸ‰ Full Control Optimization Complete!")
            print(f"   Time taken: {end_time - start_time:.1f} seconds")
            if 'best_performance' in self.results:
                perf = self.results['best_performance']
                print(f"\nğŸ† Best Performance:")
                for metric, value in perf.items():
                    print(f"   {metric}: {value:.4f}")

            if 'best_hyperparameters' in self.results:
                print(f"\nâš™ï¸ Best Hyperparameters:")
                for param, value in self.results['best_hyperparameters'].items():
                    print(f"   {param}: {value}")

        return self.results

    def _create_default_space(self) -> HyperparameterSpace:
        """å‰µå»ºé»˜èªè¶…åƒæ•¸ç©ºé–“"""
        space = HyperparameterSpace()

        if "time_series" in self.task_type or "ecg" in self.task_type:
            # æ™‚é–“åºåˆ—ç‰¹å®šåƒæ•¸
            space.add_discrete("hidden_size", [32, 64, 128, 256, 512], agent=0)
            space.add_continuous("learning_rate", 0.0001, 0.01, agent=1)
            space.add_discrete("batch_size", [16, 32, 64, 128], agent=1)
            space.add_continuous("dropout", 0.0, 0.5, agent=0)

            # æ·»åŠ é¡åˆ¥æ¬Šé‡ï¼ˆå‡è¨­9å€‹é¡åˆ¥ï¼Œé€™æ˜¯ICBEBæ•¸æ“šé›†çš„æ¨™æº–ï¼‰
            for i in range(9):  # ICBEBæœ‰9å€‹é¡åˆ¥
                space.add_continuous(f"class_weight_{i}", 0.5, 3.0, agent=2)
        else:
            # é€šç”¨åˆ†é¡åƒæ•¸
            space.add_continuous("learning_rate", 0.0001, 0.1, agent=0)
            space.add_discrete("batch_size", [16, 32, 64, 128, 256], agent=1)
            space.add_continuous("regularization", 1e-6, 1e-2, agent=2)

        return space

    def get_best_config(self) -> Dict[str, Any]:
        """ç²å–æœ€ä½³è¶…åƒæ•¸é…ç½®"""
        if self.results and 'best_hyperparameters' in self.results:
            return self.results['best_hyperparameters']
        return {}

    def get_optimization_stats(self) -> Dict[str, Any]:
        """ç²å–å„ªåŒ–çµ±è¨ˆä¿¡æ¯"""
        if not self.results:
            return {}

        stats = {
            'trials_completed': self.results.get('trials_completed', 0),
            'optimization_time': self.results.get('optimization_time', 0),
            'best_performance': self.results.get('best_performance', {}),
            'configuration': {
                'llm_enabled': self.llm_enabled,
                'mixing_strategy': self.mixing_strategy,
                'alpha': self.alpha,
                'slope_threshold': self.slope_threshold,
                'replay_buffer_size': self.replay_buffer_size,
                'learning_rate': self.learning_rate
            }
        }

        return stats


# ä¾¿åˆ©å‡½æ•¸
def full_control_optimize(X_train, y_train, X_val=None, y_val=None,
                         task_type="time_series_classification",
                         max_trials=30,
                         llm_strategy="adaptive",
                         alpha=0.3,
                         slope_threshold=0.01,
                         **kwargs):
    """
    å®Œå…¨æ§åˆ¶çš„ä¸€è¡Œå¼å„ªåŒ–å‡½æ•¸

    Args:
        X_train, y_train: è¨“ç·´æ•¸æ“š
        X_val, y_val: é©—è­‰æ•¸æ“š
        task_type: ä»»å‹™é¡å‹
        max_trials: æœ€å¤§è©¦é©—æ¬¡æ•¸
        llm_strategy: LLMç­–ç•¥
        alpha: æ··åˆæ¯”ä¾‹
        slope_threshold: æ–œç‡é–¾å€¼
        **kwargs: å…¶ä»–æ‰€æœ‰åƒæ•¸

    Returns:
        æœ€ä½³è¶…åƒæ•¸é…ç½®
    """
    optimizer = FullControlHPO(
        task_type=task_type,
        max_trials=max_trials,
        mixing_strategy=llm_strategy,
        alpha=alpha,
        slope_threshold=slope_threshold,
        **kwargs
    )

    results = optimizer.optimize(X_train, y_train, X_val, y_val)
    return optimizer.get_best_config()