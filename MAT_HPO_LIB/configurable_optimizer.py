#!/usr/bin/env python3
"""
Configurable MAT-HPO Optimizer - åˆ†å±¤ç´šè¶…åƒæ•¸å„ªåŒ–å™¨

å°ˆæ¥­ç´šå¤šæ™ºèƒ½é«”å¼·åŒ–å­¸ç¿’è¶…åƒæ•¸å„ªåŒ–ï¼Œæ”¯æ´LLMå¢å¼·å’Œåˆ†å±¤ç´šé…ç½®ï¼š
1. åˆ†å±¤ç´šåƒæ•¸è¨­å®š - æŒ‰éœ€é¡¯ç¤ºç›¸é—œåƒæ•¸
2. æ™ºèƒ½é»˜èªå€¼ - åŸºæ–¼æœ€ä½³å¯¦è¸çš„è‡ªå‹•é…ç½®
3. æ¼¸é€²å¼è¤‡é›œåº¦ - å¾ç°¡å–®åˆ°å°ˆæ¥­ç´šç²¾ç¢ºæ§åˆ¶
4. LLMå¢å¼·å„ªåŒ– - æ”¯æ´å¤šç¨®æ™ºèƒ½ç­–ç•¥
"""

from typing import Dict, Any, Optional, Union
from pathlib import Path
import warnings

from .core.llm_enhanced_optimizer import LLMEnhancedMAT_HPO_Optimizer, LLMEnhancedOptimizationConfig
from .core.enhanced_environment import TimeSeriesEnvironment
from .core.hyperparameter_space import HyperparameterSpace
from .easy_hpo import EasyHPOEnvironment


class LLMEnhancedHPO:
    """
    LLMå¢å¼·è¶…åƒæ•¸å„ªåŒ–å™¨ (LLM Enhanced Hyperparameter Optimizer)

    åŸºæ–¼å¤šæ™ºèƒ½é«”å¼·åŒ–å­¸ç¿’çš„è¶…åƒæ•¸å„ªåŒ–å™¨ï¼Œå¯é¸LLMå¢å¼·ï¼Œæä¾›åˆ†å±¤ç´šé…ç½®ï¼š
    - æ ¸å¿ƒå±¤ï¼šä»»å‹™é¡å‹å’Œè©¦é©—æ¬¡æ•¸ï¼ˆå¿…éœ€ï¼‰
    - LLMå±¤ï¼šLLMå¢å¼·ç­–ç•¥ï¼ˆå¯é¸ï¼‰
    - é…ç½®å±¤ï¼šLLMå’ŒRLè©³ç´°åƒæ•¸ï¼ˆå¯é¸ï¼‰
    """

    def __init__(self,
                 # === æ ¸å¿ƒå±¤ç´š (å¿…éœ€) ===
                 task_type: str = "time_series_classification",
                 max_trials: int = 30,

                 # === LLMå±¤ç´š (å¯é¸) ===
                 llm_enabled: bool = False,
                 llm_strategy: str = "adaptive",  # "fixed_alpha", "adaptive"

                 # === é…ç½®å±¤ç´š (å¯é¸) ===
                 llm_config: Optional[Dict[str, Any]] = None,

                 # === é€šç”¨è¨­å®š ===
                 seed: int = 42,
                 verbose: bool = True,
                 output_dir: str = "./llm_enhanced_hpo_results"):
        """
        åˆå§‹åŒ–LLMå¢å¼·HPOå„ªåŒ–å™¨

        Args:
            # æ ¸å¿ƒåƒæ•¸
            task_type: ä»»å‹™é¡å‹ ("time_series_classification", "ecg_classification", "classification")
            max_trials: æœ€å¤§è©¦é©—æ¬¡æ•¸

            # LLMæ™ºèƒ½å±¤åƒæ•¸ (åƒ…ç•¶llm_enabled=Trueæ™‚ç”Ÿæ•ˆ)
            llm_enabled: æ˜¯å¦å•Ÿç”¨LLMæ™ºèƒ½å¢å¼·
            llm_strategy: LLMç­–ç•¥
                - "fixed_alpha": å›ºå®šæ··åˆæ¯”ä¾‹ (alphaåƒæ•¸æ§åˆ¶LLM/RLæ¯”ä¾‹)
                - "adaptive": ç›£æ§RLæ€§èƒ½æ–œç‡çš„è‡ªé©æ‡‰è§¸ç™¼ (åŸºæ–¼è«–æ–‡ arXiv:2507.13712)

            # å°ˆå®¶é…ç½® (å­—å…¸å½¢å¼ï¼Œå¯é¸)
            expert_config: å°ˆå®¶ç´šåƒæ•¸å­—å…¸ï¼Œå¯åŒ…å«ï¼š
                # LLMç‰¹å®šåƒæ•¸
                - 'alpha': å›ºå®šæ··åˆæ¯”ä¾‹ï¼Œåƒ…ç”¨æ–¼fixed_alphaç­–ç•¥ (default: 0.3)
                - 'performance_threshold': RLæ€§èƒ½æ–œç‡é–¾å€¼ï¼Œç”¨æ–¼adaptiveç­–ç•¥ (default: 0.01)
                - 'llm_model': LLMæ¨¡å‹ (default: "llama3.2:3b")
                - 'llm_cooldown': LLMå†·å»æœŸ (default: 5)

                # RLç‰¹å®šåƒæ•¸
                - 'buffer_size': ç¶“é©—å›æ”¾å¤§å° (default: 1000)
                - 'learning_rate': RLå­¸ç¿’ç‡ (default: 0.001)
                - 'gamma': æŠ˜æ‰£å› å­ (default: 0.99)
                - 'tau': è»Ÿæ›´æ–°ä¿‚æ•¸ (default: 0.001)

                # å…¶ä»–åƒæ•¸
                - 'device': è¨ˆç®—è¨­å‚™ (default: "auto")

            # é€šç”¨è¨­å®š
            seed: éš¨æ©Ÿç¨®å­
            verbose: é¡¯ç¤ºè©³ç´°ä¿¡æ¯
            output_dir: è¼¸å‡ºç›®éŒ„
        """

        # === æ ¸å¿ƒåƒæ•¸ ===
        self.task_type = task_type
        self.max_trials = max_trials
        self.seed = seed
        self.verbose = verbose
        self.output_dir = Path(output_dir)

        # === LLMåƒæ•¸è™•ç† ===
        self.llm_enabled = llm_enabled
        self.llm_strategy = llm_strategy if llm_enabled else None

        # === LLMé…ç½®è™•ç† ===
        self.llm_config = llm_config or {}

        # === é…ç½®é»˜èªå€¼ ===
        self._setup_default_configs()

        # === é©—è­‰é…ç½® ===
        self._validate_configuration()

        # ç¢ºä¿è¼¸å‡ºç›®éŒ„å­˜åœ¨
        self.output_dir.mkdir(parents=True, exist_ok=True)

        # åˆå§‹åŒ–çµ„ä»¶
        self.environment = None
        self.hyperparameter_space = None
        self.optimizer = None
        self.results = None

        # é¡¯ç¤ºé…ç½®æ‘˜è¦
        self._display_configuration()

    def _setup_default_configs(self):
        """è¨­å®šé…ç½®é»˜èªå€¼"""

        # LLMé»˜èªå€¼
        if self.llm_enabled:
            llm_defaults = {
                'model': 'llama3.2:3b',
                'base_url': 'http://localhost:11434',
                'alpha': 0.3,
                'slope_threshold': 0.01,
                'cooldown': 5,
                'min_episodes': 5
            }
            # åˆä½µç”¨æˆ¶é…ç½®å’Œé»˜èªå€¼
            self.llm_config = {**llm_defaults, **self.llm_config}
        else:
            self.llm_config = None

        # RLé»˜èªå€¼ (æ ¹æ“šä»»å‹™é¡å‹èª¿æ•´)
        if "time_series" in self.task_type or "ecg" in self.task_type:
            # æ™‚é–“åºåˆ—ä»»å‹™çš„å„ªåŒ–é»˜èªå€¼
            rl_defaults = {
                'buffer_size': 1000,
                'learning_rate': 0.001,
                'gamma': 0.99,
                'tau': 0.001,
                'hidden_dim': 256,
                'batch_size': 64
            }
        else:
            # é€šç”¨åˆ†é¡ä»»å‹™çš„é»˜èªå€¼
            rl_defaults = {
                'buffer_size': 800,
                'learning_rate': 0.002,
                'gamma': 0.95,
                'tau': 0.002,
                'hidden_dim': 128,
                'batch_size': 32
            }

        # æ‡‰ç”¨ç”¨æˆ¶è‡ªå®šç¾©çš„RLåƒæ•¸
        if self.llm_config:
            rl_overrides = {k: v for k, v in self.llm_config.items()
                           if k in rl_defaults and v is not None}
            self.rl_config = {**rl_defaults, **rl_overrides}
            # è¨­å‚™è¨­å®š
            self.device = self.llm_config.get('device', 'auto')
        else:
            self.rl_config = rl_defaults
            self.device = 'auto'

    def _validate_configuration(self):
        """é©—è­‰é…ç½®çš„æœ‰æ•ˆæ€§"""

        # é©—è­‰ä»»å‹™é¡å‹
        valid_tasks = ["time_series_classification", "ecg_classification", "classification", "image_classification"]
        if self.task_type not in valid_tasks:
            warnings.warn(f"Unknown task_type '{self.task_type}', using default hyperparameter space")

        # é©—è­‰LLMç­–ç•¥
        if self.llm_enabled:
            valid_strategies = ["fixed_alpha", "adaptive"]
            if self.llm_strategy not in valid_strategies:
                raise ValueError(f"Invalid llm_strategy '{self.llm_strategy}'. Must be one of {valid_strategies}")

        # é©—è­‰æ•¸å€¼ç¯„åœ
        if self.llm_enabled and self.llm_config:
            if not (0 <= self.llm_config['alpha'] <= 1):
                raise ValueError(f"alpha must be between 0 and 1, got {self.llm_config['alpha']}")
            if self.llm_config['performance_threshold'] <= 0:
                raise ValueError(f"performance_threshold must be positive, got {self.llm_config['performance_threshold']}")

    def _display_configuration(self):
        """é¡¯ç¤ºç•¶å‰é…ç½®æ‘˜è¦"""
        if not self.verbose:
            return

        print(f"ğŸš€ Intelligent HPO Configuration:")
        print(f"   Task: {self.task_type}")
        print(f"   Max trials: {self.max_trials}")

        if self.llm_enabled:
            print(f"   ğŸ¤– LLM: âœ… Enabled ({self.llm_strategy})")
            if self.llm_strategy == "fixed_alpha":
                print(f"      Alpha: {self.llm_config['alpha']} ({self.llm_config['alpha']*100:.0f}% LLM + {(1-self.llm_config['alpha'])*100:.0f}% RL)")
            elif self.llm_strategy == "adaptive":
                print(f"      Performance threshold: {self.llm_config['performance_threshold']}")
            print(f"      Model: {self.llm_config['model']}")
        else:
            print(f"   ğŸ¤– LLM: âŒ Disabled (Pure RL)")

        print(f"   ğŸ§  RL: buffer={self.rl_config['buffer_size']}, lr={self.rl_config['learning_rate']}")
        print(f"   ğŸ“ Output: {self.output_dir}")

    def optimize(self,
                 X_train, y_train,
                 X_val=None, y_val=None,
                 X_test=None, y_test=None,
                 custom_space: Optional[HyperparameterSpace] = None) -> Dict[str, Any]:
        """
        é‹è¡Œå„ªåŒ–

        Args:
            X_train, y_train: è¨“ç·´æ•¸æ“š
            X_val, y_val: é©—è­‰æ•¸æ“š (å¯é¸ï¼Œæœƒè‡ªå‹•åˆ†å‰²)
            X_test, y_test: æ¸¬è©¦æ•¸æ“š (å¯é¸)
            custom_space: è‡ªå®šç¾©è¶…åƒæ•¸ç©ºé–“ (å¯é¸)

        Returns:
            å„ªåŒ–çµæœå­—å…¸
        """

        # è‡ªå‹•åˆ†å‰²é©—è­‰é›†
        if X_val is None and y_val is None:
            from sklearn.model_selection import train_test_split
            import numpy as np
            X_train, X_val, y_train, y_val = train_test_split(
                X_train, y_train, test_size=0.2, random_state=self.seed,
                stratify=y_train if len(np.unique(y_train)) > 1 else None
            )
            if self.verbose:
                print(f"ğŸ“Š Auto-split: train={len(X_train)}, val={len(X_val)}")

        # è¨­ç½®ç’°å¢ƒ - ä½¿ç”¨å·²ç¶“å·¥ä½œçš„EasyHPOEnvironment
        self.environment = EasyHPOEnvironment(
            task_type=self.task_type,
            dataset_name=f"ConfigurableMAT_HPO_{self.task_type}"
        )
        self.environment.set_data(X_train, y_train, X_val, y_val, X_test, y_test)
        self.environment.load_data()

        # è¨­ç½®è¶…åƒæ•¸ç©ºé–“
        if custom_space:
            self.hyperparameter_space = custom_space
        else:
            self.hyperparameter_space = self._create_smart_space()

        # å‰µå»ºå„ªåŒ–é…ç½®
        config = self._create_optimization_config()

        # å‰µå»ºå„ªåŒ–å™¨
        self.optimizer = LLMEnhancedMAT_HPO_Optimizer(
            environment=self.environment,
            hyperparameter_space=self.hyperparameter_space,
            config=config
        )

        if self.verbose:
            print(f"\\nâš¡ Starting optimization...")

        # é‹è¡Œå„ªåŒ–
        import time
        start_time = time.time()
        self.results = self.optimizer.optimize()
        end_time = time.time()

        # æ·»åŠ çµ±è¨ˆä¿¡æ¯
        self.results['optimization_time'] = end_time - start_time
        self.results['trials_completed'] = self.max_trials
        self.results['configuration'] = self._get_config_summary()

        if self.verbose:
            self._display_results()

        return self.results

    def _create_smart_space(self) -> HyperparameterSpace:
        """æ ¹æ“šä»»å‹™é¡å‹å‰µå»ºæ™ºèƒ½è¶…åƒæ•¸ç©ºé–“ï¼Œç¢ºä¿æ¯å€‹agentéƒ½æœ‰åƒæ•¸"""
        space = HyperparameterSpace()

        if "time_series" in self.task_type or "ecg" in self.task_type:
            # æ™‚é–“åºåˆ—å„ªåŒ–ç©ºé–“
            space.add_discrete("hidden_size", [64, 128, 256, 512], agent=0)
            space.add_continuous("dropout", 0.0, 0.5, agent=0)

            space.add_continuous("learning_rate", 0.0001, 0.01, agent=1)
            space.add_discrete("batch_size", [16, 32, 64, 128], agent=1)

            # ç¢ºä¿agent 2æœ‰åƒæ•¸
            space.add_continuous("regularization", 1e-6, 1e-3, agent=2)

            # ECGç‰¹å®šçš„é¡åˆ¥æ¬Šé‡
            if "ecg" in self.task_type and hasattr(self.environment, 'y_train'):
                import numpy as np
                num_classes = len(np.unique(self.environment.y_train))
                if num_classes > 2:
                    for i in range(min(num_classes, 9)):  # æœ€å¤š9å€‹é¡åˆ¥æ¬Šé‡
                        space.add_continuous(f"class_weight_{i}", 0.5, 3.0, agent=2)

        elif "image" in self.task_type:
            # åœ–åƒåˆ†é¡ç©ºé–“
            space.add_discrete("filters", [32, 64, 128, 256], agent=0)
            space.add_discrete("kernel_size", [3, 5, 7], agent=0)

            space.add_continuous("learning_rate", 0.0001, 0.001, agent=1)
            space.add_discrete("batch_size", [32, 64, 128], agent=1)

            space.add_continuous("regularization", 1e-6, 1e-3, agent=2)

        else:
            # é€šç”¨åˆ†é¡ç©ºé–“
            space.add_continuous("learning_rate", 0.0001, 0.1, agent=0)
            space.add_continuous("regularization", 1e-6, 1e-2, agent=0)

            space.add_discrete("batch_size", [16, 32, 64, 128], agent=1)
            space.add_continuous("momentum", 0.8, 0.99, agent=1)

            space.add_continuous("weight_decay", 1e-6, 1e-3, agent=2)
            space.add_discrete("optimizer_type", ['adam', 'sgd'], agent=2)

        return space

    def _create_optimization_config(self) -> LLMEnhancedOptimizationConfig:
        """å‰µå»ºå„ªåŒ–é…ç½®"""

        # ä½¿ç”¨é¡ä¼¼EasyHPOçš„ç°¡åŒ–é…ç½®æ–¹å¼
        config_params = {
            'max_steps': self.max_trials,
            'replay_buffer_size': self.rl_config['buffer_size'],
            'verbose': self.verbose,
            'dataset_name': self.environment.dataset_name,
            'task_description': self.environment.get_llm_context()
        }

        # LLMåƒæ•¸ (åƒ…ç•¶å•Ÿç”¨æ™‚æ·»åŠ )
        if self.llm_enabled:
            config_params.update({
                'enable_llm': True,
                'llm_model': self.llm_config['model'],
                'llm_base_url': self.llm_config['base_url'],
                'mixing_strategy': self.llm_strategy,
                'alpha': self.llm_config['alpha'],
                'slope_threshold': self.llm_config['slope_threshold'],
                'llm_cooldown_episodes': self.llm_config['cooldown'],
                'min_episodes_before_llm': self.llm_config['min_episodes']
            })
        else:
            config_params['enable_llm'] = False

        return LLMEnhancedOptimizationConfig(**config_params)

    def _get_config_summary(self) -> Dict[str, Any]:
        """ç²å–é…ç½®æ‘˜è¦"""
        summary = {
            'task_type': self.task_type,
            'max_trials': self.max_trials,
            'llm_enabled': self.llm_enabled,
            'rl_config': self.rl_config
        }

        if self.llm_enabled:
            summary['llm_config'] = {
                'strategy': self.llm_strategy,
                'model': self.llm_config['model'],
                'alpha': self.llm_config['alpha'],
                'slope_threshold': self.llm_config['slope_threshold']
            }

        return summary

    def _display_results(self):
        """é¡¯ç¤ºå„ªåŒ–çµæœ"""
        if not self.results:
            return

        print(f"\\nğŸ‰ Optimization Complete!")
        print(f"   Time: {self.results['optimization_time']:.1f} seconds")

        if 'best_performance' in self.results:
            print(f"\\nğŸ† Best Performance:")
            for metric, value in self.results['best_performance'].items():
                print(f"   {metric}: {value:.4f}")

        if 'best_hyperparameters' in self.results:
            print(f"\\nâš™ï¸  Best Hyperparameters:")
            for param, value in self.results['best_hyperparameters'].items():
                if isinstance(value, float):
                    print(f"   {param}: {value:.6f}")
                else:
                    print(f"   {param}: {value}")

    def get_best_config(self) -> Dict[str, Any]:
        """ç²å–æœ€ä½³é…ç½®"""
        if self.results and 'best_hyperparameters' in self.results:
            return self.results['best_hyperparameters']
        return {}

    def get_config_summary(self) -> Dict[str, Any]:
        """ç²å–é…ç½®æ‘˜è¦"""
        if self.results and 'configuration' in self.results:
            return self.results['configuration']
        return self._get_config_summary()


# === ä¾¿åˆ©å‡½æ•¸ ===

def optimize_with_llm(X_train, y_train, X_val=None, y_val=None,
                     task_type="time_series_classification",
                     strategy="adaptive",
                     max_trials=30,
                     **expert_config):
    """
    LLMå¢å¼·çš„ä¸€è¡Œå¼å„ªåŒ–

    Args:
        X_train, y_train: è¨“ç·´æ•¸æ“š
        X_val, y_val: é©—è­‰æ•¸æ“š
        task_type: ä»»å‹™é¡å‹
        strategy: LLMç­–ç•¥ ("fixed_alpha", "adaptive")
        max_trials: æœ€å¤§è©¦é©—æ¬¡æ•¸
        **expert_config: å°ˆå®¶é…ç½®åƒæ•¸

    Returns:
        æœ€ä½³è¶…åƒæ•¸é…ç½®å­—å…¸
    """
    optimizer = IntelligentHPO(
        task_type=task_type,
        max_trials=max_trials,
        llm_enabled=True,
        llm_strategy=strategy,
        expert_config=expert_config
    )
    results = optimizer.optimize(X_train, y_train, X_val, y_val)
    return optimizer.get_best_config()


def optimize_pure_rl(X_train, y_train, X_val=None, y_val=None,
                    task_type="time_series_classification",
                    max_trials=30,
                    **advanced_config):
    """
    ç´”å¤šæ™ºèƒ½é«”å¼·åŒ–å­¸ç¿’å„ªåŒ–

    Args:
        X_train, y_train: è¨“ç·´æ•¸æ“š
        X_val, y_val: é©—è­‰æ•¸æ“š
        task_type: ä»»å‹™é¡å‹
        max_trials: æœ€å¤§è©¦é©—æ¬¡æ•¸
        **expert_config: å°ˆå®¶é…ç½®åƒæ•¸

    Returns:
        æœ€ä½³è¶…åƒæ•¸é…ç½®å­—å…¸
    """
    optimizer = IntelligentHPO(
        task_type=task_type,
        max_trials=max_trials,
        llm_enabled=False,
        expert_config=expert_config
    )
    results = optimizer.optimize(X_train, y_train, X_val, y_val)
    return optimizer.get_best_config()