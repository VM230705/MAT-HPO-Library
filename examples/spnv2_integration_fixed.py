"""
SPNV2 Integration Example for MAT-HPO Library

âœ… ä¿®å¾©ç‰ˆæœ¬ï¼šå±•ç¤ºå¦‚ä½•æ­£ç¢ºä½¿ç”¨ä¿®å¾©å¾Œçš„ MAT-HPO Library

This example demonstrates how to integrate SPNV2 with the fixed MAT-HPO Library,
ensuring proper step tracking, consistent WandB logging, and val_f1_macro optimization.
"""

import os
import sys
import torch
import numpy as np
from typing import Dict, Any, Optional

# Add MAT_HPO_Library to path
sys.path.append('/home/vm230705/NTSC_Project_v2/MAT_HPO_Library')

from MAT_HPO_LIB.core.multi_agent_optimizer import MAT_HPO_Optimizer
from MAT_HPO_LIB.core.llm_enhanced_optimizer import LLMEnhancedMAT_HPO_Optimizer, LLMEnhancedOptimizationConfig
from MAT_HPO_LIB.core.evaluation_criteria import ModelSaveCriteria, OptimizationTarget, create_spnv2_criteria
from MAT_HPO_LIB.core.hyperparameter_space import HyperparameterSpace
from MAT_HPO_LIB.utils.config import OptimizationConfig
from MAT_HPO_LIB.utils.spnv2_config import SPNV2ConfigLoader, SPNV2HPOConfig
from MAT_HPO_LIB.utils.wandb_standards import WandBStandards

class SPNV2Environment:
    """
    SPNV2 ç’°å¢ƒé©é…å™¨
    
    é€™å€‹é¡åˆ¥å°‡ SPNV2 çš„è¨“ç·´é‚è¼¯é©é…åˆ° MAT-HPO Library çš„ç’°å¢ƒæ¥å£
    """
    
    def __init__(self, dataset_name: str = "ICBEB", fold: int = 1):
        self.dataset_name = dataset_name
        self.fold = fold
        self.name = f"SPNV2-{dataset_name}-Fold{fold}"
        
        # è¿½è¹¤æœ€ä½³çµæœ
        self.best_val_f1 = float('-inf')
        self.best_step = -1
        self.best_hyperparams = None
        self.best_metrics = {}
        
    def reset(self) -> torch.Tensor:
        """é‡ç½®ç’°å¢ƒç‹€æ…‹"""
        # è¿”å›åˆå§‹ç‹€æ…‹å¼µé‡
        return torch.zeros(1, 12)  # 9 class weights + 3 other params
    
    def step(self, hyperparams: Dict[str, Any]) -> tuple:
        """
        åŸ·è¡Œä¸€æ­¥å„ªåŒ–
        
        Args:
            hyperparams: è¶…åƒæ•¸å­—å…¸
            
        Returns:
            tuple: (reward, metrics, done)
        """
        # æ¨¡æ“¬ SPNV2 è¨“ç·´éç¨‹
        # åœ¨å¯¦éš›ä½¿ç”¨ä¸­ï¼Œé€™è£¡æœƒèª¿ç”¨ SPNV2 çš„è¨“ç·´ä»£ç¢¼
        
        # æ¨¡æ“¬è¨“ç·´çµæœ
        val_f1 = np.random.uniform(0.6, 0.8)  # æ¨¡æ“¬ val_f1_macro
        val_acc = np.random.uniform(0.7, 0.9)
        test_f1 = np.random.uniform(0.6, 0.8)
        test_acc = np.random.uniform(0.7, 0.9)
        test_auc = np.random.uniform(0.8, 0.95)
        test_gmean = np.random.uniform(0.6, 0.8)
        
        # å‰µå»ºæŒ‡æ¨™å­—å…¸
        metrics = {
            'val_f1': val_f1,
            'val_acc': val_acc,
            'val_precision': np.random.uniform(0.6, 0.8),
            'val_recall': np.random.uniform(0.6, 0.8),
            'test_f1': test_f1,
            'test_acc': test_acc,
            'test_precision': np.random.uniform(0.6, 0.8),
            'test_recall': np.random.uniform(0.6, 0.8),
            'test_auc': test_auc,
            'test_gmean': test_gmean
        }
        
        # è¨ˆç®—çå‹µï¼ˆåŸºæ–¼ val_f1_macroï¼‰
        reward = val_f1
        
        # æª¢æŸ¥æ˜¯å¦å®Œæˆ
        done = False
        
        return reward, metrics, done

def create_spnv2_hyperparameter_space() -> HyperparameterSpace:
    """
    å‰µå»º SPNV2 å°ˆç”¨çš„è¶…åƒæ•¸ç©ºé–“
    
    Returns:
        HyperparameterSpace: é…ç½®å¥½çš„è¶…åƒæ•¸ç©ºé–“
    """
    space = HyperparameterSpace()
    
    # Agent 0: Class weights (9 classes)
    for i in range(9):
        space.add_continuous(f'class_weight_{i}', 0.1, 2.0, agent=0)
    
    # Agent 1: Architecture parameters
    space.add_continuous('hidden_size', 100, 500, agent=1)
    
    # Agent 2: Training parameters
    space.add_continuous('batch_size', 16, 64, agent=2)
    space.add_continuous('learning_rate', 1e-5, 1e-2, agent=2)
    
    return space

def run_spnv2_hpo_example():
    """
    é‹è¡Œ SPNV2 HPO ç¯„ä¾‹
    
    å±•ç¤ºå¦‚ä½•ä½¿ç”¨ä¿®å¾©å¾Œçš„ MAT-HPO Library é€²è¡Œ SPNV2 å„ªåŒ–
    """
    print("ğŸš€ é–‹å§‹ SPNV2 HPO ç¯„ä¾‹")
    print("=" * 50)
    
    # 1. å‰µå»ºç’°å¢ƒ
    environment = SPNV2Environment(dataset_name="ICBEB", fold=1)
    print(f"âœ… ç’°å¢ƒå‰µå»ºå®Œæˆ: {environment.name}")
    
    # 2. å‰µå»ºè¶…åƒæ•¸ç©ºé–“
    hyperparameter_space = create_spnv2_hyperparameter_space()
    print(f"âœ… è¶…åƒæ•¸ç©ºé–“å‰µå»ºå®Œæˆ: {len(hyperparameter_space.parameters)} å€‹åƒæ•¸")
    
    # 3. å‰µå»ºè©•ä¼°æ¨™æº–ï¼ˆä»¥ val_f1_macro ç‚ºä¸»è¦ç›®æ¨™ï¼‰
    evaluation_criteria = create_spnv2_criteria()
    print(f"âœ… è©•ä¼°æ¨™æº–å‰µå»ºå®Œæˆ: {evaluation_criteria.primary_target.value}")
    
    # 4. å‰µå»ºé…ç½®
    config = OptimizationConfig(
        max_steps=20,  # è¼ƒå°‘çš„æ­¥é©Ÿç”¨æ–¼ç¯„ä¾‹
        device='cuda:0' if torch.cuda.is_available() else 'cpu',
        verbose=True
    )
    print(f"âœ… é…ç½®å‰µå»ºå®Œæˆ: {config.max_steps} æ­¥é©Ÿ")
    
    # 5. å‰µå»ºå„ªåŒ–å™¨
    optimizer = MAT_HPO_Optimizer(
        environment=environment,
        hyperparameter_space=hyperparameter_space,
        config=config,
        evaluation_criteria=evaluation_criteria,
        output_dir="./spnv2_hpo_example_results"
    )
    print("âœ… å„ªåŒ–å™¨å‰µå»ºå®Œæˆ")
    
    # 6. é‹è¡Œå„ªåŒ–
    print("\nğŸ”„ é–‹å§‹å„ªåŒ–...")
    results = optimizer.optimize()
    
    # 7. é¡¯ç¤ºçµæœ
    print("\nğŸ“Š å„ªåŒ–çµæœ:")
    print(f"æœ€ä½³æ­¥é©Ÿ: {results['optimization_stats']['best_step']}")
    print(f"æœ€ä½³ val_f1: {results['best_performance']['val_f1']:.4f}")
    print(f"æœ€ä½³ test_f1: {results['best_performance']['test_f1']:.4f}")
    print(f"ç¸½æ™‚é–“: {results['optimization_stats']['total_time']:.2f} ç§’")
    
    # 8. é©—è­‰è¼¸å‡ºæª”æ¡ˆ
    print("\nğŸ“ è¼¸å‡ºæª”æ¡ˆæª¢æŸ¥:")
    output_dir = "./spnv2_hpo_example_results"
    files_to_check = [
        'best_hyperparams.json',
        'optimization_results.json',
        'step_log.jsonl',
        'RL_model0.pt',
        'RL_model1.pt',
        'RL_model2.pt',
        'RL_model_input.pt',
        'CNNLSTM_model_hyp.npy'
    ]
    
    for filename in files_to_check:
        filepath = os.path.join(output_dir, filename)
        if os.path.exists(filepath):
            print(f"âœ… {filename}")
        else:
            print(f"âŒ {filename} (ç¼ºå¤±)")
    
    # 9. æª¢æŸ¥ best_hyperparams.json å…§å®¹
    best_hyp_path = os.path.join(output_dir, 'best_hyperparams.json')
    if os.path.exists(best_hyp_path):
        import json
        with open(best_hyp_path, 'r') as f:
            best_data = json.load(f)
        
        print(f"\nğŸ” best_hyperparams.json å…§å®¹æª¢æŸ¥:")
        print(f"  Step: {best_data.get('step', 'N/A')}")
        print(f"  Optimization Target: {best_data.get('optimization_target', 'N/A')}")
        print(f"  Primary Score: {best_data.get('performance', {}).get('primary_score', 'N/A')}")
        print(f"  Timestamp: {best_data.get('timestamp', 'N/A')}")
        
        # é©—è­‰ step æ˜¯å¦æ­£ç¢º
        if best_data.get('step') == results['optimization_stats']['best_step']:
            print("âœ… Step è¿½è¹¤æ­£ç¢º")
        else:
            print("âŒ Step è¿½è¹¤éŒ¯èª¤")
    
    print("\nğŸ‰ SPNV2 HPO ç¯„ä¾‹å®Œæˆï¼")

def run_llm_enhanced_example():
    """
    é‹è¡Œ LLM Enhanced HPO ç¯„ä¾‹
    
    å±•ç¤ºå¦‚ä½•ä½¿ç”¨ LLM Enhanced ç‰ˆæœ¬
    """
    print("\nğŸ¤– é–‹å§‹ LLM Enhanced HPO ç¯„ä¾‹")
    print("=" * 50)
    
    # 1. å‰µå»ºç’°å¢ƒå’Œè¶…åƒæ•¸ç©ºé–“
    environment = SPNV2Environment(dataset_name="ICBEB", fold=2)
    hyperparameter_space = create_spnv2_hyperparameter_space()
    
    # 2. å‰µå»º LLM Enhanced é…ç½®
    llm_config = LLMEnhancedOptimizationConfig(
        max_steps=10,  # è¼ƒå°‘çš„æ­¥é©Ÿç”¨æ–¼ç¯„ä¾‹
        enable_llm=False,  # é—œé–‰ LLM ç”¨æ–¼ç°¡å–®ç¯„ä¾‹
        device='cuda:0' if torch.cuda.is_available() else 'cpu',
        verbose=True
    )
    
    # 3. å‰µå»º LLM Enhanced å„ªåŒ–å™¨
    optimizer = LLMEnhancedMAT_HPO_Optimizer(
        environment=environment,
        hyperparameter_space=hyperparameter_space,
        config=llm_config,
        evaluation_criteria=create_spnv2_criteria(),
        output_dir="./spnv2_llm_hpo_example_results"
    )
    
    # 4. é‹è¡Œå„ªåŒ–
    print("ğŸ”„ é–‹å§‹ LLM Enhanced å„ªåŒ–...")
    results = optimizer.optimize()
    
    # 5. é¡¯ç¤ºçµæœ
    print("\nğŸ“Š LLM Enhanced å„ªåŒ–çµæœ:")
    print(f"æœ€ä½³æ­¥é©Ÿ: {results['optimization_stats']['best_step']}")
    print(f"æœ€ä½³ val_f1: {results['best_performance']['val_f1']:.4f}")
    
    print("\nğŸ‰ LLM Enhanced HPO ç¯„ä¾‹å®Œæˆï¼")

if __name__ == "__main__":
    # é‹è¡ŒåŸºæœ¬ç¯„ä¾‹
    run_spnv2_hpo_example()
    
    # é‹è¡Œ LLM Enhanced ç¯„ä¾‹
    run_llm_enhanced_example()
    
    print("\n" + "=" * 50)
    print("ğŸ¯ æ‰€æœ‰ç¯„ä¾‹å®Œæˆï¼")
    print("ğŸ“‹ ä¸»è¦ä¿®å¾©å…§å®¹ï¼š")
    print("  âœ… æ­£ç¢ºçš„ step è¿½è¹¤")
    print("  âœ… çµ±ä¸€çš„ WandB è¨˜éŒ„æ ¼å¼")
    print("  âœ… ä»¥ val_f1_macro ç‚ºä¸»è¦ç›®æ¨™")
    print("  âœ… éˆæ´»çš„è©•ä¼°æ¨™æº–")
    print("  âœ… ç”¨æˆ¶å¯è‡ªè¨‚çš„é…ç½®ç³»çµ±")
