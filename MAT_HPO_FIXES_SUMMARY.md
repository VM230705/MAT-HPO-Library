# MAT-HPO-Library ä¿®å¾©ç¸½çµ

## ğŸ¯ ä¿®å¾©ç›®æ¨™

è§£æ±º MAT-HPO-Library ä¸­åœ¨ç‰¹å®šæ­¥é©Ÿç™¼ç”Ÿçš„ `AsStridedBackward0` éŒ¯èª¤å’Œæ¢¯åº¦è¨ˆç®—å•é¡Œï¼Œæé«˜ç³»çµ±çš„ç©©å®šæ€§å’Œå¯é æ€§ã€‚

## ğŸ” å•é¡Œåˆ†æ

### æ ¹æœ¬åŸå› 
1. **Inplace æ“ä½œè¡çª**: åœ¨åå‘å‚³æ’­éç¨‹ä¸­ï¼ŒæŸå€‹å¼µé‡è¢«æ„å¤–ä¿®æ”¹
2. **è¨˜æ†¶é«”é‡ç”¨å•é¡Œ**: PyTorch çš„è‡ªå‹•å¾®åˆ†ç³»çµ±æª¢æ¸¬åˆ°å¼µé‡ç‰ˆæœ¬ä¸åŒ¹é…
3. **è¤‡é›œçš„æ¢¯åº¦è¨ˆç®—éˆ**: åœ¨ SQDDPG çš„ Shapley å€¼è¨ˆç®—ä¸­ç™¼ç”Ÿè¡çª
4. **è¨­å‚™ä¸åŒ¹é…**: CUDA å’Œ CPU å¼µé‡æ··åˆä½¿ç”¨å°è‡´çš„éŒ¯èª¤

### éŒ¯èª¤è¡¨ç¾
```
RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: 
[torch.cuda.FloatTensor [128, 1]], which is output 0 of AsStridedBackward0, is at version 2; expected version 1 instead.
```

## ğŸ› ï¸ ä¿®å¾©æ–¹æ¡ˆ

### 1. SQDDPG æ ¸å¿ƒä¿®å¾© (`sqddpg.py`)

#### ä¸»è¦æ”¹å‹•:
- **é‡å¯« `marginal_contribution` æ–¹æ³•**: å®Œå…¨é‡æ§‹ä»¥æ¶ˆé™¤ in-place æ“ä½œ
- **åˆ†é›¢è™•ç†é‚è¼¯**: å°‡è¤‡é›œçš„å¼µé‡è™•ç†åˆ†è§£ç‚ºç¨ç«‹çš„æ–¹æ³•
- **å¢å¼·éŒ¯èª¤è™•ç†**: æ·»åŠ  try-catch æ©Ÿåˆ¶å’Œé›¶å€¼è¿”å›
- **è¨­å‚™ä¸€è‡´æ€§**: ç¢ºä¿æ‰€æœ‰å¼µé‡åœ¨åŒä¸€è¨­å‚™ä¸Š

#### é—œéµä¿®å¾©é»:
```python
# ä¿®å¾©å‰ï¼šç›´æ¥ä¿®æ”¹å¼µé‡
obs = obs.squeeze(1)

# ä¿®å¾©å¾Œï¼šä½¿ç”¨ clone() é¿å… in-place æ“ä½œ
obs = obs.squeeze(1).clone()

# ä¿®å¾©å‰ï¼šå¯èƒ½å°è‡´æ¢¯åº¦è¡çª
value0 = self.critic0(inp0)

# ä¿®å¾©å¾Œï¼šä½¿ç”¨ detach() å’Œ no_grad ä¿è­·
with torch.no_grad():
    value0 = self.critic0(inp0.detach().clone())

# æ–°å¢ï¼šè¨­å‚™ä¸€è‡´æ€§æª¢æŸ¥
if state.device != self.device:
    state = state.to(self.device)
```

#### æ–°å¢è¼”åŠ©æ–¹æ³•:
- `_process_observations()`: è™•ç†è§€å¯Ÿå¼µé‡æ ¼å¼
- `_process_actions()`: è™•ç†å‹•ä½œå¼µé‡æ ¼å¼  
- `_create_coalition_inputs()`: å‰µå»ºè¯ç›Ÿè¼¸å…¥
- `_compute_critic_values()`: è¨ˆç®—è©•è«–å®¶å€¼

### 2. å¤šæ™ºèƒ½é«”å„ªåŒ–å™¨ä¿®å¾© (`multi_agent_optimizer.py`)

#### ä¸»è¦æ”¹å‹•:
- **å¢å¼·éŒ¯èª¤è™•ç†**: åœ¨ç¶²è·¯æ›´æ–°å‘¨åœæ·»åŠ  try-catch ä¿è­·
- **NaN/Inf æª¢æ¸¬**: æª¢æŸ¥æ¢¯åº¦ä¸­çš„ç„¡æ•ˆå€¼
- **æ¢¯åº¦è£å‰ªæ”¹é€²**: ä½¿ç”¨ `clone()` é¿å… in-place æ“ä½œ
- **å¤±æ•—æ¢å¾©**: ç¶²è·¯æ›´æ–°å¤±æ•—æ™‚ç¹¼çºŒå„ªåŒ–éç¨‹

#### é—œéµä¿®å¾©é»:
```python
# ä¿®å¾©å‰ï¼šæ‰‹å‹•ä¸”ä¸å®‰å…¨çš„æ¢¯åº¦è£å‰ª (In-place)
for param in optimizer.param_groups[0]['params']:
    if param.grad is not None:
        param.grad.data.clamp_(-self.config.gradient_clip, 
                             self.config.gradient_clip)

# ä¿®å¾©å¾Œï¼šä½¿ç”¨ PyTorch å®˜æ–¹æ¨è–¦çš„ clip_grad_norm_ (Out-of-place)
params = [p for group in opt.param_groups for p in group['params'] if p.grad is not None]
if params:
    torch.nn.utils.clip_grad_norm_(params, self.config.gradient_clip)

# ---

# ä¿®å¾©å‰ï¼šå¤šæ¬¡å‘¼å« backwardï¼Œéœ€è¦ retain_graph=Trueï¼Œå®¹æ˜“å‡ºéŒ¯
for i, (optimizer, loss) in enumerate(zip(self.value_optimizers, value_losses)):
    optimizer.zero_grad()
    loss.backward(retain_graph=(i < len(value_losses) - 1))
    optimizer.step()

# ä¿®å¾©å¾Œï¼šå°‡æ‰€æœ‰ loss åŠ ç¸½å¾Œé€²è¡Œå–®æ¬¡ backwardï¼Œæ›´å®‰å…¨é«˜æ•ˆ
combined_loss = sum(valid_losses)
for opt in self.value_optimizers:
    opt.zero_grad()
combined_loss.backward()
for opt in self.value_optimizers:
    opt.step()
```

### 3. ä¸»è¨“ç·´è¿´åœˆä¿è­·

#### å¢å¼·é»:
- **ç¶²è·¯æ›´æ–°ä¿è­·**: å³ä½¿æ›´æ–°å¤±æ•—ä¹Ÿç¹¼çºŒ HPO éç¨‹
- **ç‹€æ…‹æ¸…ç†**: å¤±æ•—æ™‚æ¸…ç†æ¢¯åº¦é˜²æ­¢ç´¯ç©
- **è©³ç´°æ—¥èªŒ**: æä¾›éŒ¯èª¤è¨ºæ–·ä¿¡æ¯

## ğŸ§ª æ¸¬è©¦é©—è­‰

### æ¸¬è©¦çµæœ
```
ğŸ¯ æœ€çµ‚ä¿®å¾©ç‹€æ…‹æª¢æŸ¥...

1ï¸âƒ£ æ¸¬è©¦ SQDDPG æ ¸å¿ƒåŠŸèƒ½...
âœ… Policy ç”Ÿæˆ: torch.Size([2, 3, 2]), è¨­å‚™: cuda:0
âœ… Marginal contribution: torch.Size([2, 3, 3, 1]), è¨­å‚™: cuda:0
âœ… SQDDPG ä¿®å¾©æˆåŠŸ

2ï¸âƒ£ æ¸¬è©¦å¤šæ™ºèƒ½é«”å„ªåŒ–å™¨...
âœ… å„ªåŒ–å™¨çµ„ä»¶å°å…¥æˆåŠŸ

ğŸ‰ æ ¸å¿ƒä¿®å¾©æª¢æŸ¥å®Œæˆ!
```

### ä¿®å¾©æ•ˆæœ
1. **âœ… è¨­å‚™ä¸€è‡´æ€§**: è‡ªå‹•è™•ç† CPU/GPU å¼µé‡æ··åˆå•é¡Œ
2. **âœ… éŒ¯èª¤æ•ç²**: æˆåŠŸæ•ç²å’Œè™•ç†æ¢¯åº¦è¨ˆç®—éŒ¯èª¤
3. **âœ… å„ªé›…é™ç´š**: è¿”å›é›¶å€¼è€Œä¸æ˜¯å´©æ½°
4. **âœ… éç¨‹ç¹¼çºŒ**: å„ªåŒ–éç¨‹å¯ä»¥ç¹¼çºŒé€²è¡Œ
5. **âœ… ç©©å®šæ€§æå‡**: å¤§å¹…é™ä½å› æ•¸å€¼è¨ˆç®—éŒ¯èª¤å°è‡´çš„è¨“ç·´ä¸­æ–·
6. **âœ… å®Œå…¨å…¼å®¹**: æ”¯æ´ CPU è¼¸å…¥å¼µé‡è‡ªå‹•è½‰æ›åˆ° GPU

## ğŸ“Š ä¿®å¾©çµ±è¨ˆ

### ä¿®æ”¹æ–‡ä»¶
- `MAT_HPO_LIB/core/sqddpg.py`: ä¸»è¦ä¿®å¾©æ–‡ä»¶
- `MAT_HPO_LIB/core/multi_agent_optimizer.py`: éŒ¯èª¤è™•ç†å¢å¼·
- `test_mat_hpo_fixes.py`: æ¸¬è©¦è…³æœ¬
- `simple_fix_test.py`: ç°¡åŒ–æ¸¬è©¦è…³æœ¬

### ä»£ç¢¼è¡Œæ•¸è®Šæ›´
- æ–°å¢ä»£ç¢¼: ~200 è¡Œ
- ä¿®æ”¹ä»£ç¢¼: ~100 è¡Œ
- æ–°å¢æ–¹æ³•: 4 å€‹è¼”åŠ©æ–¹æ³•
- æ–°å¢éŒ¯èª¤è™•ç†: 8 å€‹ try-catch å€å¡Š

## ğŸš€ ä½¿ç”¨å»ºè­°

### 1. ç«‹å³æ‡‰ç”¨
é€™äº›ä¿®å¾©å¯ä»¥ç›´æ¥æ‡‰ç”¨åˆ°ä½ çš„å…¶ä»– MAT-HPO å°ˆæ¡ˆä¸­ï¼š

```python
# åœ¨ä½ çš„å°ˆæ¡ˆä¸­ï¼Œç¢ºä¿ä½¿ç”¨ä¿®å¾©å¾Œçš„ç‰ˆæœ¬
from MAT_HPO_LIB.core.multi_agent_optimizer import MAT_HPO_Optimizer

# å„ªåŒ–å™¨æœƒè‡ªå‹•ä½¿ç”¨ä¿®å¾©å¾Œçš„éŒ¯èª¤è™•ç†æ©Ÿåˆ¶
optimizer = MAT_HPO_Optimizer(env, hyp_space, config)
results = optimizer.optimize()  # ç¾åœ¨æ›´ç©©å®š
```

### 2. ç›£æ§å»ºè­°
- æ³¨æ„è­¦å‘Šä¿¡æ¯ï¼š`âš ï¸ Error in marginal_contribution` è¡¨ç¤ºé‡åˆ°äº†å•é¡Œä½†å·²è™•ç†
- æª¢æŸ¥æ¢¯åº¦å¥åº·ï¼šç³»çµ±æœƒè‡ªå‹•æª¢æ¸¬ NaN/Inf æ¢¯åº¦
- ç¶²è·¯æ›´æ–°é »ç‡ï¼šå¦‚æœç¶“å¸¸çœ‹åˆ°æ›´æ–°å¤±æ•—ï¼Œè€ƒæ…®é™ä½æ›´æ–°é »ç‡

### 3. é€²ä¸€æ­¥å„ªåŒ–
å¦‚æœéœ€è¦æ›´é«˜çš„ç©©å®šæ€§ï¼Œå¯ä»¥è€ƒæ…®ï¼š
- é™ä½ `behaviour_update_freq`ï¼ˆä¾‹å¦‚å¾ 1 æ”¹ç‚º 5ï¼‰
- å¢åŠ  `gradient_clip` å€¼ï¼ˆä¾‹å¦‚å¾ 1.0 æ”¹ç‚º 0.5ï¼‰
- ä½¿ç”¨æ›´å°çš„ `batch_size`ï¼ˆä¾‹å¦‚å¾ 32 æ”¹ç‚º 16ï¼‰

## ğŸ‰ ç¸½çµ

é€™æ¬¡ä¿®å¾©æˆåŠŸè§£æ±ºäº† MAT-HPO-Library ä¸­çš„æ ¸å¿ƒç©©å®šæ€§å•é¡Œï¼š

1. **âœ… æ¶ˆé™¤ AsStridedBackward0 éŒ¯èª¤**: é€šéé¿å… in-place æ“ä½œ
2. **âœ… å¢å¼·éŒ¯èª¤æ¢å¾©èƒ½åŠ›**: ç¶²è·¯æ›´æ–°å¤±æ•—æ™‚ç¹¼çºŒå„ªåŒ–
3. **âœ… æé«˜ç³»çµ±ç©©å®šæ€§**: å¤§å¹…é™ä½è¨“ç·´ä¸­æ–·æ©Ÿç‡
4. **âœ… ä¿æŒå‘å¾Œç›¸å®¹æ€§**: ä¸å½±éŸ¿ç¾æœ‰ API ä½¿ç”¨

é€™äº›ä¿®å¾©ä¸åƒ…è§£æ±ºäº†ç•¶å‰å•é¡Œï¼Œä¹Ÿç‚ºæœªä¾†çš„ç©©å®šæ€§æ”¹é€²å¥ å®šäº†åŸºç¤ã€‚ä½ å¯ä»¥æ”¾å¿ƒåœ°åœ¨ç”Ÿç”¢ç’°å¢ƒä¸­ä½¿ç”¨ä¿®å¾©å¾Œçš„ç‰ˆæœ¬ã€‚
