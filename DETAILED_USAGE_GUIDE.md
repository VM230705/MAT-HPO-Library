# MAT-HPO Library - è©³ç´°ä½¿ç”¨èªªæ˜

## ğŸ“– ç›®éŒ„
1. [æ¦‚è¿°](#æ¦‚è¿°)
2. [å®‰è£èˆ‡è¨­å®š](#å®‰è£èˆ‡è¨­å®š)
3. [æ ¸å¿ƒæ¦‚å¿µ](#æ ¸å¿ƒæ¦‚å¿µ)
4. [å¿«é€Ÿé–‹å§‹](#å¿«é€Ÿé–‹å§‹)
5. [è©³ç´°æ•™å­¸](#è©³ç´°æ•™å­¸)
6. [é€²éšåŠŸèƒ½](#é€²éšåŠŸèƒ½)
7. [å®Œæ•´ç¯„ä¾‹](#å®Œæ•´ç¯„ä¾‹)
8. [å¸¸è¦‹å•é¡Œ](#å¸¸è¦‹å•é¡Œ)
9. [æœ€ä½³å¯¦è¸](#æœ€ä½³å¯¦è¸)

## æ¦‚è¿°

MAT-HPO (Multi-Agent Transformer Hyperparameter Optimization) æ˜¯ä¸€å€‹å¼·å¤§è€Œéˆæ´»çš„è¶…åƒæ•¸å„ªåŒ–å‡½å¼åº«ï¼Œå°ˆç‚ºè¤‡é›œçš„æ©Ÿå™¨å­¸ç¿’å•é¡Œè¨­è¨ˆã€‚å®ƒä½¿ç”¨å¤šä»£ç†å¼·åŒ–å­¸ç¿’æ–¹æ³•ï¼Œè®“ä¸åŒçš„ä»£ç†å°ˆé–€è² è²¬ä¸åŒé¡å‹çš„è¶…åƒæ•¸å„ªåŒ–ã€‚

### ğŸŒŸ ä¸»è¦ç‰¹é»

- **å¤šä»£ç†æ¶æ§‹**: ä¸‰å€‹å°ˆé–€çš„ä»£ç†åˆ†åˆ¥å„ªåŒ–ä¸åŒé¡å‹çš„è¶…åƒæ•¸
- **Transformerç¶²è·¯**: ä½¿ç”¨å…ˆé€²çš„Transformeræ¶æ§‹é€²è¡Œæ™ºèƒ½è¶…åƒæ•¸é¸æ“‡
- **å½ˆæ€§æ•´åˆ**: æ˜“æ–¼æ•´åˆåˆ°ä»»ä½•æ©Ÿå™¨å­¸ç¿’ç®¡é“ä¸­
- **å¼·å¤§çš„é©—è­‰**: å®Œæ•´çš„éŒ¯èª¤æª¢æŸ¥å’Œåƒæ•¸é©—è­‰æ©Ÿåˆ¶
- **è±å¯Œçš„åŠŸèƒ½**: æ”¯æ´æª¢æŸ¥é»ã€å›èª¿å‡½æ•¸ã€è‡ªè¨‚ç´„æŸç­‰
- **è©³ç´°æ—¥èªŒ**: å®Œæ•´çš„å„ªåŒ–æ­·å²è¨˜éŒ„å’Œæ•ˆèƒ½è¿½è¹¤

### ğŸ¯ é©ç”¨å ´æ™¯

- æ·±åº¦å­¸ç¿’æ¨¡å‹è¶…åƒæ•¸èª¿æ•´
- å‚³çµ±æ©Ÿå™¨å­¸ç¿’ç®—æ³•å„ªåŒ–
- AutoMLç³»çµ±é–‹ç™¼
- ç ”ç©¶å¯¦é©—è‡ªå‹•åŒ–
- å¤šç›®æ¨™å„ªåŒ–å•é¡Œ

## å®‰è£èˆ‡è¨­å®š

### åŸºæœ¬éœ€æ±‚

```bash
# Python 3.8+
pip install torch numpy scikit-learn tqdm
```

### å®‰è£å‡½å¼åº«

```bash
# æ–¹æ³•1: ç›´æ¥å¾åŸå§‹ç¢¼ä½¿ç”¨
cd path/to/MAT_HPO_LIB
export PYTHONPATH="${PYTHONPATH}:$(pwd)"

# æ–¹æ³•2: å®‰è£ç‚ºPythonåŒ… (æ¨è–¦)
pip install -e .
```

### é©—è­‰å®‰è£

```python
from MAT_HPO_LIB import MAT_HPO_Optimizer, BaseEnvironment, HyperparameterSpace
print("âœ… MAT-HPO Library successfully imported!")
```

## æ ¸å¿ƒæ¦‚å¿µ

### 1. å¤šä»£ç†åˆ†å·¥

MAT-HPOä½¿ç”¨ä¸‰å€‹å°ˆé–€çš„ä»£ç†ä¾†å„ªåŒ–ä¸åŒé¡å‹çš„è¶…åƒæ•¸ï¼š

```python
# å…¸å‹çš„ä»£ç†åˆ†å·¥
Agent 0: å•é¡Œç‰¹å®šåƒæ•¸ (é¡åˆ¥æ¬Šé‡ã€æ­£è¦åŒ–ã€é ˜åŸŸç›¸é—œåƒæ•¸)
Agent 1: æ¨¡å‹æ¶æ§‹åƒæ•¸ (éš±è—å±¤å¤§å°ã€å±¤æ•¸ã€ç¶²è·¯çµæ§‹)  
Agent 2: è¨“ç·´åƒæ•¸ (æ‰¹æ¬¡å¤§å°ã€å­¸ç¿’ç‡ã€å„ªåŒ–å™¨è¨­å®š)
```

### 2. æ ¸å¿ƒçµ„ä»¶

#### BaseEnvironment (åŸºç¤ç’°å¢ƒ)
```python
class MyEnvironment(BaseEnvironment):
    def load_data(self):
        # åŠ è¼‰å’Œé è™•ç†æ•¸æ“š
        return data
    
    def create_model(self, hyperparams):
        # æ ¹æ“šè¶…åƒæ•¸å‰µå»ºæ¨¡å‹
        return model
    
    def train_evaluate(self, model, hyperparams):
        # è¨“ç·´æ¨¡å‹ä¸¦è©•ä¼°æ•ˆèƒ½
        return {'accuracy': 0.95, 'f1': 0.93}
    
    def compute_reward(self, metrics):
        # è¨ˆç®—çå‹µä¿¡è™Ÿ
        return metrics['f1'] * 0.6 + metrics['accuracy'] * 0.4
```

#### HyperparameterSpace (è¶…åƒæ•¸ç©ºé–“)
```python
space = HyperparameterSpace(
    agent0_params=['class_weight_0', 'class_weight_1'],
    agent1_params=['hidden_size', 'num_layers'],
    agent2_params=['batch_size', 'learning_rate'],
    bounds={
        'class_weight_0': (0.1, 5.0),      # æ•¸å€¼ç¯„åœ
        'hidden_size': (64, 512),          # æ•´æ•¸ç¯„åœ
        'learning_rate': (1e-5, 1e-2),     # å°æ•¸åˆ†å¸ƒ
        'optimizer': ['adam', 'sgd']       # é¡åˆ¥é¸æ“‡
    },
    param_types={
        'class_weight_0': float,
        'hidden_size': int,
        'learning_rate': 'log_uniform',
        'optimizer': str
    }
)
```

#### MAT_HPO_Optimizer (å„ªåŒ–å™¨)
```python
optimizer = MAT_HPO_Optimizer(
    environment=environment,
    hyperparameter_space=hyperparameter_space,
    config=OptimizationConfig(
        max_steps=100,
        use_cuda=True,
        verbose=True
    )
)
```

## å¿«é€Ÿé–‹å§‹

### 30ç§’å¿«é€Ÿç¯„ä¾‹

```python
from MAT_HPO_LIB import *
import torch
import torch.nn as nn
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, f1_score

class SimpleNNEnvironment(BaseEnvironment):
    def __init__(self):
        super().__init__(name="SimpleNN")
        self.X_train = None
        self.X_val = None
        self.y_train = None
        self.y_val = None
    
    def load_data(self):
        # ç”Ÿæˆç¤ºä¾‹æ•¸æ“š
        X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)
        self.X_train, self.X_val, self.y_train, self.y_val = train_test_split(
            X, y, test_size=0.2, random_state=42
        )
        return {"loaded": True}
    
    def create_model(self, hyperparams):
        model = nn.Sequential(
            nn.Linear(20, hyperparams['hidden_size']),
            nn.ReLU(),
            nn.Linear(hyperparams['hidden_size'], 2)
        )
        return model
    
    def train_evaluate(self, model, hyperparams):
        # ç°¡åŒ–çš„è¨“ç·´éç¨‹
        optimizer = torch.optim.Adam(model.parameters(), lr=hyperparams['learning_rate'])
        criterion = nn.CrossEntropyLoss()
        
        # å¿«é€Ÿè¨“ç·´
        X_train = torch.FloatTensor(self.X_train)
        y_train = torch.LongTensor(self.y_train)
        X_val = torch.FloatTensor(self.X_val)
        y_val = torch.LongTensor(self.y_val)
        
        for epoch in range(10):  # å¿«é€Ÿç¤ºä¾‹ï¼Œåªè¨“ç·´10å€‹epoch
            optimizer.zero_grad()
            outputs = model(X_train)
            loss = criterion(outputs, y_train)
            loss.backward()
            optimizer.step()
        
        # è©•ä¼°
        model.eval()
        with torch.no_grad():
            val_outputs = model(X_val)
            _, predicted = torch.max(val_outputs.data, 1)
            accuracy = accuracy_score(y_val, predicted)
            f1 = f1_score(y_val, predicted)
        
        return {'accuracy': accuracy, 'f1': f1}
    
    def compute_reward(self, metrics):
        return metrics['f1'] * 0.7 + metrics['accuracy'] * 0.3

# å®šç¾©è¶…åƒæ•¸ç©ºé–“
space = HyperparameterSpace(
    agent0_params=[],  # é€™å€‹ä¾‹å­ä¸­æ²’æœ‰å•é¡Œç‰¹å®šåƒæ•¸
    agent1_params=['hidden_size'],
    agent2_params=['learning_rate'],
    bounds={
        'hidden_size': (32, 256),
        'learning_rate': (1e-4, 1e-2)
    },
    param_types={
        'hidden_size': int,
        'learning_rate': 'log_uniform'
    }
)

# å‰µå»ºå’Œé‹è¡Œå„ªåŒ–å™¨
environment = SimpleNNEnvironment()
optimizer = MAT_HPO_Optimizer(environment, space, DefaultConfigs.quick_test())
results = optimizer.optimize()

print(f"ğŸ‰ æœ€ä½³è¶…åƒæ•¸: {results['best_hyperparameters']}")
print(f"ğŸ† æœ€ä½³æ•ˆèƒ½: F1={results['best_performance']['f1']:.4f}")
```

## è©³ç´°æ•™å­¸

### 1. ç’°å¢ƒè¨­è¨ˆè©³è§£

#### æ•¸æ“šåŠ è¼‰ç­–ç•¥
```python
class AdvancedEnvironment(BaseEnvironment):
    def load_data(self):
        # æ–¹å¼1: å¾æ–‡ä»¶åŠ è¼‰
        train_data = pd.read_csv('train.csv')
        val_data = pd.read_csv('val.csv')
        
        # æ–¹å¼2: ä½¿ç”¨æ•¸æ“šå¢å¼·
        train_data = self.apply_augmentation(train_data)
        
        # æ–¹å¼3: æ”¯æ´å¤šç¨®æ•¸æ“šæ ¼å¼
        if self.data_type == 'images':
            return self.load_image_data()
        elif self.data_type == 'text':
            return self.load_text_data()
        else:
            return self.load_tabular_data()
    
    def load_image_data(self):
        # åœ–åƒæ•¸æ“šåŠ è¼‰é‚è¼¯
        transform = transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                               std=[0.229, 0.224, 0.225])
        ])
        
        train_dataset = ImageFolder('data/train', transform=transform)
        val_dataset = ImageFolder('data/val', transform=transform)
        
        return {
            'train_dataset': train_dataset,
            'val_dataset': val_dataset
        }
```

#### æ¨¡å‹å‰µå»ºæœ€ä½³å¯¦è¸
```python
def create_model(self, hyperparams):
    # æ ¹æ“šå•é¡Œé¡å‹é¸æ“‡åŸºç¤æ¶æ§‹
    if self.problem_type == 'classification':
        return self._create_classifier(hyperparams)
    elif self.problem_type == 'regression':
        return self._create_regressor(hyperparams)
    elif self.problem_type == 'time_series':
        return self._create_time_series_model(hyperparams)

def _create_classifier(self, hyperparams):
    # æ”¯æ´å‹•æ…‹æ¶æ§‹
    layers = []
    input_size = self.feature_dim
    
    # å‹•æ…‹æ·»åŠ éš±è—å±¤
    for i in range(hyperparams.get('num_layers', 2)):
        hidden_size = hyperparams.get(f'hidden_size_{i}', 128)
        layers.extend([
            nn.Linear(input_size, hidden_size),
            nn.ReLU(),
            nn.Dropout(hyperparams.get('dropout', 0.5))
        ])
        input_size = hidden_size
    
    # è¼¸å‡ºå±¤
    layers.append(nn.Linear(input_size, self.num_classes))
    
    return nn.Sequential(*layers)
```

#### è©•ä¼°å’Œçå‹µè¨­è¨ˆ
```python
def train_evaluate(self, model, hyperparams):
    # å®Œæ•´çš„è¨“ç·´å¾ªç’°
    train_loader = DataLoader(self.train_dataset, 
                             batch_size=hyperparams['batch_size'],
                             shuffle=True)
    val_loader = DataLoader(self.val_dataset, 
                           batch_size=hyperparams['batch_size'])
    
    # å„ªåŒ–å™¨é¸æ“‡
    optimizer_type = hyperparams.get('optimizer', 'adam')
    if optimizer_type == 'adam':
        optimizer = torch.optim.Adam(model.parameters(), 
                                   lr=hyperparams['learning_rate'])
    elif optimizer_type == 'sgd':
        optimizer = torch.optim.SGD(model.parameters(), 
                                  lr=hyperparams['learning_rate'],
                                  momentum=hyperparams.get('momentum', 0.9))
    
    # å­¸ç¿’ç‡èª¿åº¦å™¨
    scheduler = torch.optim.lr_scheduler.StepLR(
        optimizer, 
        step_size=hyperparams.get('lr_step_size', 10),
        gamma=hyperparams.get('lr_gamma', 0.1)
    )
    
    # è¨“ç·´å¾ªç’°
    best_val_loss = float('inf')
    patience_counter = 0
    
    for epoch in range(hyperparams.get('epochs', 50)):
        # è¨“ç·´éšæ®µ
        model.train()
        train_loss = 0.0
        for batch_x, batch_y in train_loader:
            optimizer.zero_grad()
            outputs = model(batch_x)
            loss = F.cross_entropy(outputs, batch_y)
            loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            if hyperparams.get('gradient_clip', 0) > 0:
                torch.nn.utils.clip_grad_norm_(model.parameters(), 
                                             hyperparams['gradient_clip'])
            
            optimizer.step()
            train_loss += loss.item()
        
        # é©—è­‰éšæ®µ
        model.eval()
        val_loss, predictions, targets = self._validate(model, val_loader)
        
        # æ—©åœæª¢æŸ¥
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            patience_counter = 0
            best_model = copy.deepcopy(model.state_dict())
        else:
            patience_counter += 1
            
        if patience_counter >= hyperparams.get('early_stopping_patience', 10):
            model.load_state_dict(best_model)
            break
        
        scheduler.step()
    
    # è¨ˆç®—æœ€çµ‚æŒ‡æ¨™
    metrics = self._compute_metrics(predictions, targets)
    metrics['training_epochs'] = epoch + 1
    metrics['final_train_loss'] = train_loss / len(train_loader)
    metrics['final_val_loss'] = best_val_loss
    
    return metrics

def compute_reward(self, metrics):
    # å¤šç›®æ¨™çå‹µå‡½æ•¸
    primary_score = metrics.get('f1', metrics.get('accuracy', 0))
    
    # æ•ˆç‡çå‹µ (è¨“ç·´æ™‚é–“è¶ŠçŸ­è¶Šå¥½)
    efficiency_bonus = max(0, 1.0 - metrics.get('training_epochs', 50) / 50)
    
    # ç©©å®šæ€§çå‹µ (é©—è­‰æå¤±è¶Šä½è¶Šå¥½)
    stability_bonus = max(0, 1.0 - metrics.get('final_val_loss', 1.0))
    
    # è¤‡é›œåº¦æ‡²ç½° (é¿å…éåº¦è¤‡é›œçš„æ¨¡å‹)
    complexity_penalty = 0.1 * metrics.get('model_params', 0) / 1000000  # æ¯ç™¾è¬åƒæ•¸0.1çš„æ‡²ç½°
    
    total_reward = (primary_score * 0.7 + 
                   efficiency_bonus * 0.15 + 
                   stability_bonus * 0.1 + 
                   complexity_penalty * 0.05)
    
    return total_reward
```

### 2. è¶…åƒæ•¸ç©ºé–“è¨­è¨ˆ

#### æ”¯æ´çš„åƒæ•¸é¡å‹
```python
# æ•¸å€¼åƒæ•¸
'learning_rate': (1e-5, 1e-2)        # ç·šæ€§åˆ†å¸ƒ
'learning_rate': 'log_uniform'        # å°æ•¸åˆ†å¸ƒ (æ¨è–¦ç”¨æ–¼å­¸ç¿’ç‡)

# æ•´æ•¸åƒæ•¸  
'hidden_size': (64, 512)              # æ•´æ•¸ç¯„åœ
'num_layers': (1, 10)                 # å±¤æ•¸

# é¡åˆ¥åƒæ•¸
'optimizer': ['adam', 'sgd', 'rmsprop', 'adamw']
'activation': ['relu', 'gelu', 'swish', 'leaky_relu']

# å¸ƒæ—åƒæ•¸
'use_batch_norm': bool                # è‡ªå‹•è¨­ç‚º (False, True)
'use_dropout': (False, True)          # æ˜ç¢ºæŒ‡å®š
```

#### è¤‡é›œè¶…åƒæ•¸ç©ºé–“ç¯„ä¾‹
```python
# å®Œæ•´çš„æ·±åº¦å­¸ç¿’è¶…åƒæ•¸ç©ºé–“
space = HyperparameterSpace(
    agent0_params=[
        'class_weight_0', 'class_weight_1', 'class_weight_2',  # é¡åˆ¥å¹³è¡¡
        'data_augmentation_strength',                          # æ•¸æ“šå¢å¼·
        'regularization_strength'                              # æ­£è¦åŒ–
    ],
    agent1_params=[
        'backbone_type',           # ä¸»å¹¹ç¶²è·¯é¡å‹
        'hidden_size_1',          # ç¬¬ä¸€éš±è—å±¤
        'hidden_size_2',          # ç¬¬äºŒéš±è—å±¤  
        'num_layers',             # ç¸½å±¤æ•¸
        'activation_type',        # æ¿€æ´»å‡½æ•¸
        'use_batch_norm',         # æ˜¯å¦ä½¿ç”¨BN
        'dropout_rate'            # Dropoutæ¯”ç‡
    ],
    agent2_params=[
        'batch_size',             # æ‰¹æ¬¡å¤§å°
        'learning_rate',          # å­¸ç¿’ç‡
        'optimizer_type',         # å„ªåŒ–å™¨
        'lr_schedule',            # å­¸ç¿’ç‡èª¿åº¦
        'weight_decay',           # æ¬Šé‡è¡°æ¸›
        'gradient_clip_norm',     # æ¢¯åº¦è£å‰ª
        'early_stopping_patience' # æ—©åœè€å¿ƒ
    ],
    bounds={
        # Agent 0: å•é¡Œç‰¹å®šåƒæ•¸
        'class_weight_0': (0.1, 10.0),
        'class_weight_1': (0.1, 10.0), 
        'class_weight_2': (0.1, 10.0),
        'data_augmentation_strength': (0.0, 1.0),
        'regularization_strength': (1e-6, 1e-2),
        
        # Agent 1: æ¶æ§‹åƒæ•¸
        'backbone_type': ['resnet', 'densenet', 'efficientnet'],
        'hidden_size_1': (64, 1024),
        'hidden_size_2': (32, 512),
        'num_layers': (2, 8),
        'activation_type': ['relu', 'gelu', 'swish'],
        'use_batch_norm': (False, True),
        'dropout_rate': (0.0, 0.8),
        
        # Agent 2: è¨“ç·´åƒæ•¸
        'batch_size': (8, 128),
        'learning_rate': (1e-5, 1e-1),
        'optimizer_type': ['adam', 'adamw', 'sgd', 'rmsprop'],
        'lr_schedule': ['constant', 'step', 'cosine', 'exponential'],
        'weight_decay': (1e-6, 1e-2),
        'gradient_clip_norm': (0.1, 10.0),
        'early_stopping_patience': (5, 50)
    },
    param_types={
        # Agent 0
        'class_weight_0': float,
        'class_weight_1': float,
        'class_weight_2': float,
        'data_augmentation_strength': float,
        'regularization_strength': 'log_uniform',
        
        # Agent 1  
        'backbone_type': str,
        'hidden_size_1': int,
        'hidden_size_2': int,
        'num_layers': int,
        'activation_type': str,
        'use_batch_norm': bool,
        'dropout_rate': float,
        
        # Agent 2
        'batch_size': int,
        'learning_rate': 'log_uniform',
        'optimizer_type': str,
        'lr_schedule': str,
        'weight_decay': 'log_uniform',
        'gradient_clip_norm': float,
        'early_stopping_patience': int
    },
    default_values={
        'learning_rate': 1e-3,
        'batch_size': 32,
        'optimizer_type': 'adam',
        'activation_type': 'relu'
    },
    parameter_descriptions={
        'learning_rate': 'å­¸ç¿’ç‡ï¼Œæ§åˆ¶æ¨¡å‹æ›´æ–°æ­¥é•·',
        'batch_size': 'æ‰¹æ¬¡å¤§å°ï¼Œå½±éŸ¿æ¢¯åº¦ä¼°è¨ˆè³ªé‡',
        'hidden_size_1': 'ç¬¬ä¸€éš±è—å±¤ç¥ç¶“å…ƒæ•¸é‡'
    }
)
```

### 3. å„ªåŒ–é…ç½®å’Œå›èª¿

#### å„ªåŒ–é…ç½®è©³è§£
```python
from MAT_HPO_LIB.utils.config import OptimizationConfig, DefaultConfigs

# è‡ªå®šç¾©é…ç½®
config = OptimizationConfig(
    # æ ¸å¿ƒè¨­å®š
    max_steps=200,                    # æœ€å¤§å„ªåŒ–æ­¥æ•¸
    replay_buffer_size=2000,          # ç¶“é©—å›æ”¾ç·©è¡å€å¤§å°
    batch_size=64,                    # SQDDPGæ‰¹æ¬¡å¤§å°
    
    # å­¸ç¿’ç‡è¨­å®š
    policy_learning_rate=1e-4,        # ç­–ç•¥ç¶²è·¯å­¸ç¿’ç‡
    value_learning_rate=1e-3,         # åƒ¹å€¼ç¶²è·¯å­¸ç¿’ç‡
    
    # æ›´æ–°é »ç‡
    behaviour_update_freq=5,          # è¡Œç‚ºç­–ç•¥æ›´æ–°é »ç‡
    critic_update_times=2,            # è©•åƒ¹è€…æ›´æ–°æ¬¡æ•¸
    
    # è¨­å‚™è¨­å®š
    gpu_device=0,                     # GPUè¨­å‚™ç·¨è™Ÿ
    use_cuda=True,                    # æ˜¯å¦ä½¿ç”¨CUDA
    
    # æ—¥èªŒå’Œä¿å­˜
    save_interval=20,                 # æ¨¡å‹ä¿å­˜é–“éš”
    log_interval=1,                   # æ—¥èªŒè¼¸å‡ºé–“éš”
    verbose=True,                     # è©³ç´°è¼¸å‡º
    
    # æ—©åœè¨­å®š
    early_stop_patience=30,           # æ—©åœè€å¿ƒå€¼
    early_stop_threshold=1e-4,        # æ—©åœé–¾å€¼
    
    # é€²éšè¨­å®š
    gradient_clip=1.0,                # æ¢¯åº¦è£å‰ª
    target_update_tau=0.005,          # ç›®æ¨™ç¶²è·¯æ›´æ–°ç‡
    noise_std=0.1,                    # æ¢ç´¢å™ªéŸ³æ¨™æº–å·®
    
    # éš¨æ©Ÿç¨®å­
    seed=42,                          # éš¨æ©Ÿç¨®å­
    deterministic=True                # ç¢ºå®šæ€§åŸ·è¡Œ
)

# æˆ–ä½¿ç”¨é è¨­é…ç½®
config = DefaultConfigs.standard()   # æ¨™æº–é…ç½®
config = DefaultConfigs.quick_test()  # å¿«é€Ÿæ¸¬è©¦
config = DefaultConfigs.extensive()  # å»£æ³›æœç´¢
config = DefaultConfigs.cpu_only()   # CPUå°ˆç”¨
```

#### å›èª¿å‡½æ•¸ç³»çµ±
```python
def step_callback(env, hyperparams, metrics, reward):
    """æ¯æ­¥åŸ·è¡Œçš„å›èª¿å‡½æ•¸"""
    print(f"Step {env.current_step}: Reward = {reward:.4f}")
    
    # è¨˜éŒ„åˆ°å¤–éƒ¨ç³»çµ±
    wandb.log({
        'reward': reward,
        'f1_score': metrics.get('f1', 0),
        'step': env.current_step
    })
    
    # è‡ªå®šç¾©æ—©åœæ¢ä»¶
    if reward > 0.95:
        print("ğŸ¯ é”åˆ°ç›®æ¨™æ•ˆèƒ½ï¼Œå»ºè­°æ—©åœ")

def epoch_callback(env, epoch, model, metrics):
    """æ¯å€‹epochåŸ·è¡Œçš„å›èª¿å‡½æ•¸"""
    if epoch % 10 == 0:
        print(f"Epoch {epoch}: Loss = {metrics.get('loss', 0):.4f}")

# æ·»åŠ å›èª¿
environment.add_step_callback(step_callback)
environment.add_epoch_callback(epoch_callback)
```

### 4. æª¢æŸ¥é»å’Œæ¢å¾©

```python
# å•Ÿç”¨æª¢æŸ¥é»
environment = MyEnvironment(
    checkpoint_dir="./checkpoints/experiment_1",
    verbose=True,
    save_history=True
)

# é‹è¡Œå„ªåŒ–
optimizer = MAT_HPO_Optimizer(environment, space, config, 
                             output_dir="./results/experiment_1")
results = optimizer.optimize()

# æ¢å¾©å„ªåŒ– (å¦‚æœä¸­æ–·)
if os.path.exists("./checkpoints/experiment_1"):
    environment.load_checkpoint("./checkpoints/experiment_1")
    optimizer.load_checkpoint("./checkpoints/experiment_1")
    results = optimizer.optimize()  # ç¹¼çºŒå„ªåŒ–
```

## é€²éšåŠŸèƒ½

### 1. è‡ªè¨‚ç´„æŸå‡½æ•¸

```python
def model_size_constraint(hyperparams):
    """é™åˆ¶æ¨¡å‹å¤§å°çš„ç´„æŸå‡½æ•¸"""
    total_params = hyperparams['hidden_size_1'] * hyperparams['hidden_size_2']
    if total_params > 100000:  # é™åˆ¶åœ¨10è¬åƒæ•¸ä»¥å…§
        # ç­‰æ¯”ä¾‹ç¸®å°
        scale = (100000 / total_params) ** 0.5
        hyperparams['hidden_size_1'] = int(hyperparams['hidden_size_1'] * scale)
        hyperparams['hidden_size_2'] = int(hyperparams['hidden_size_2'] * scale)
    return hyperparams

def batch_lr_constraint(hyperparams):
    """æ‰¹æ¬¡å¤§å°å’Œå­¸ç¿’ç‡çš„ç´„æŸ"""
    # å¤§æ‰¹æ¬¡ä½¿ç”¨å¤§å­¸ç¿’ç‡
    if hyperparams['batch_size'] > 64:
        hyperparams['learning_rate'] = max(hyperparams['learning_rate'], 1e-3)
    return hyperparams

# æ·»åŠ ç´„æŸ
space.add_constraint(model_size_constraint)
space.add_constraint(batch_lr_constraint)
```

### 2. å¤šç›®æ¨™å„ªåŒ–

```python
class MultiObjectiveEnvironment(BaseEnvironment):
    def compute_reward(self, metrics):
        # Paretoå‰æ²¿å„ªåŒ–
        accuracy = metrics.get('accuracy', 0)
        inference_speed = metrics.get('inference_speed', 0)
        model_size = metrics.get('model_size', 1000000)
        
        # æ­¸ä¸€åŒ–æŒ‡æ¨™
        norm_accuracy = accuracy  # å·²ç¶“æ˜¯0-1ç¯„åœ
        norm_speed = min(inference_speed / 1000, 1.0)  # æ­¸ä¸€åŒ–åˆ°0-1
        norm_size = max(0, 1.0 - model_size / 10000000)  # è¶Šå°è¶Šå¥½
        
        # åŠ æ¬Šçµ„åˆ (å¯æ ¹æ“šéœ€æ±‚èª¿æ•´æ¬Šé‡)
        weights = [0.5, 0.3, 0.2]  # [æº–ç¢ºç‡, é€Ÿåº¦, å¤§å°]
        total_reward = (weights[0] * norm_accuracy + 
                       weights[1] * norm_speed + 
                       weights[2] * norm_size)
        
        return total_reward
```

### 3. åˆ†ä½ˆå¼å„ªåŒ–

```python
import torch.distributed as dist
import torch.multiprocessing as mp

class DistributedEnvironment(BaseEnvironment):
    def __init__(self, rank, world_size):
        super().__init__(name=f"DistributedEnv-{rank}")
        self.rank = rank
        self.world_size = world_size
        
    def train_evaluate(self, model, hyperparams):
        # åˆ†ä½ˆå¼è¨“ç·´
        model = torch.nn.parallel.DistributedDataParallel(model)
        
        # ä½¿ç”¨åˆ†ä½ˆå¼æ•¸æ“šåŠ è¼‰å™¨
        sampler = torch.utils.data.distributed.DistributedSampler(
            self.train_dataset, num_replicas=self.world_size, rank=self.rank
        )
        
        train_loader = DataLoader(
            self.train_dataset,
            batch_size=hyperparams['batch_size'],
            sampler=sampler
        )
        
        # åˆ†ä½ˆå¼è¨“ç·´é‚è¼¯
        return self._distributed_training(model, train_loader, hyperparams)

def run_distributed_optimization(rank, world_size):
    # åˆå§‹åŒ–åˆ†ä½ˆå¼ç’°å¢ƒ
    dist.init_process_group("nccl", rank=rank, world_size=world_size)
    
    environment = DistributedEnvironment(rank, world_size)
    optimizer = MAT_HPO_Optimizer(environment, space, config)
    
    results = optimizer.optimize()
    
    # æ¸…ç†
    dist.destroy_process_group()
    return results

# å•Ÿå‹•åˆ†ä½ˆå¼å„ªåŒ–
if __name__ == '__main__':
    world_size = 4
    mp.spawn(run_distributed_optimization, args=(world_size,), nprocs=world_size)
```

### 4. è‡ªé©æ‡‰æœç´¢ç©ºé–“

```python
class AdaptiveHyperparameterSpace(HyperparameterSpace):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.performance_history = []
        
    def update_bounds_based_on_performance(self, hyperparams, performance):
        """æ ¹æ“šæ•ˆèƒ½å‹•æ…‹èª¿æ•´æœç´¢ç©ºé–“"""
        self.performance_history.append((hyperparams.copy(), performance))
        
        if len(self.performance_history) >= 10:  # è‡³å°‘10å€‹æ¨£æœ¬
            # æ‰¾å‡ºæ•ˆèƒ½å‰25%çš„é…ç½®
            sorted_history = sorted(self.performance_history, 
                                  key=lambda x: x[1], reverse=True)
            top_configs = sorted_history[:len(sorted_history)//4]
            
            # ç¸®å°æœç´¢ç¯„åœåˆ°é«˜æ•ˆèƒ½å€åŸŸ
            for param in self.agent1_params + self.agent2_params:
                if param in self.numerical_params:
                    values = [config[0][param] for config, _ in top_configs 
                             if param in config[0]]
                    if values:
                        mean_val = np.mean(values)
                        std_val = np.std(values)
                        
                        # æ›´æ–°æœç´¢ç¯„åœ (mean Â± 2*std)
                        new_min = max(self.bounds[param][0], mean_val - 2*std_val)
                        new_max = min(self.bounds[param][1], mean_val + 2*std_val)
                        
                        self.bounds[param] = (new_min, new_max)
                        print(f"ğŸ“Š èª¿æ•´ {param} æœç´¢ç¯„åœåˆ° [{new_min:.4f}, {new_max:.4f}]")
```

## å®Œæ•´ç¯„ä¾‹

### ç¯„ä¾‹1: åœ–åƒåˆ†é¡å„ªåŒ–

```python
import torch
import torch.nn as nn
import torchvision.transforms as transforms
from torchvision.datasets import CIFAR10
from torch.utils.data import DataLoader

class CIFAR10Environment(BaseEnvironment):
    def __init__(self):
        super().__init__(name="CIFAR10-Classification")
        self.num_classes = 10
        self.input_channels = 3
        
    def load_data(self):
        # æ•¸æ“šè½‰æ›
        train_transform = transforms.Compose([
            transforms.RandomCrop(32, padding=4),
            transforms.RandomHorizontalFlip(),
            transforms.ToTensor(),
            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))
        ])
        
        val_transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))
        ])
        
        # åŠ è¼‰æ•¸æ“šé›†
        train_dataset = CIFAR10(root='./data', train=True, 
                               transform=train_transform, download=True)
        val_dataset = CIFAR10(root='./data', train=False, 
                             transform=val_transform)
        
        # ä½¿ç”¨å­é›†ä»¥åŠ é€Ÿå¯¦é©—
        train_size = len(train_dataset) // 10  # ä½¿ç”¨1/10æ•¸æ“š
        val_size = len(val_dataset) // 10
        
        train_dataset = torch.utils.data.Subset(train_dataset, range(train_size))
        val_dataset = torch.utils.data.Subset(val_dataset, range(val_size))
        
        self.train_dataset = train_dataset
        self.val_dataset = val_dataset
        
        return {"train_size": train_size, "val_size": val_size}
    
    def create_model(self, hyperparams):
        """å‰µå»ºCNNæ¨¡å‹"""
        class SimpleCNN(nn.Module):
            def __init__(self, hyperparams):
                super().__init__()
                
                # å·ç©å±¤
                self.conv1 = nn.Conv2d(3, hyperparams['conv1_filters'], 3, padding=1)
                self.conv2 = nn.Conv2d(hyperparams['conv1_filters'], 
                                     hyperparams['conv2_filters'], 3, padding=1)
                
                # æ‰¹æ¬¡æ­£è¦åŒ– (å¯é¸)
                self.bn1 = nn.BatchNorm2d(hyperparams['conv1_filters'])
                self.bn2 = nn.BatchNorm2d(hyperparams['conv2_filters'])
                self.use_batch_norm = hyperparams.get('use_batch_norm', True)
                
                # å…¨é€£æ¥å±¤
                fc_input_size = hyperparams['conv2_filters'] * 8 * 8  # 32->16->8
                self.fc1 = nn.Linear(fc_input_size, hyperparams['hidden_size'])
                self.fc2 = nn.Linear(hyperparams['hidden_size'], 10)
                
                # Dropout
                self.dropout = nn.Dropout(hyperparams.get('dropout_rate', 0.5))
                
            def forward(self, x):
                # å·ç© + æ± åŒ–
                x = self.conv1(x)
                if self.use_batch_norm:
                    x = self.bn1(x)
                x = torch.relu(x)
                x = torch.max_pool2d(x, 2)
                
                x = self.conv2(x)
                if self.use_batch_norm:
                    x = self.bn2(x)
                x = torch.relu(x)
                x = torch.max_pool2d(x, 2)
                
                # å±•å¹³
                x = x.view(x.size(0), -1)
                
                # å…¨é€£æ¥å±¤
                x = self.dropout(torch.relu(self.fc1(x)))
                x = self.fc2(x)
                
                return x
        
        return SimpleCNN(hyperparams)
    
    def train_evaluate(self, model, hyperparams):
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        model.to(device)
        
        # æ•¸æ“šåŠ è¼‰å™¨
        train_loader = DataLoader(self.train_dataset, 
                                 batch_size=hyperparams['batch_size'],
                                 shuffle=True, num_workers=2)
        val_loader = DataLoader(self.val_dataset,
                               batch_size=hyperparams['batch_size'],
                               shuffle=False, num_workers=2)
        
        # å„ªåŒ–å™¨å’Œæå¤±å‡½æ•¸
        optimizer = torch.optim.Adam(model.parameters(), 
                                   lr=hyperparams['learning_rate'],
                                   weight_decay=hyperparams.get('weight_decay', 1e-4))
        criterion = nn.CrossEntropyLoss()
        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)
        
        # è¨“ç·´
        best_val_acc = 0
        epochs = hyperparams.get('epochs', 20)
        
        for epoch in range(epochs):
            # è¨“ç·´éšæ®µ
            model.train()
            train_loss = 0.0
            train_correct = 0
            train_total = 0
            
            for batch_idx, (data, targets) in enumerate(train_loader):
                data, targets = data.to(device), targets.to(device)
                
                optimizer.zero_grad()
                outputs = model(data)
                loss = criterion(outputs, targets)
                loss.backward()
                optimizer.step()
                
                train_loss += loss.item()
                _, predicted = outputs.max(1)
                train_total += targets.size(0)
                train_correct += predicted.eq(targets).sum().item()
            
            # é©—è­‰éšæ®µ
            model.eval()
            val_loss = 0.0
            val_correct = 0
            val_total = 0
            
            with torch.no_grad():
                for data, targets in val_loader:
                    data, targets = data.to(device), targets.to(device)
                    outputs = model(data)
                    loss = criterion(outputs, targets)
                    
                    val_loss += loss.item()
                    _, predicted = outputs.max(1)
                    val_total += targets.size(0)
                    val_correct += predicted.eq(targets).sum().item()
            
            # è¨ˆç®—æº–ç¢ºç‡
            train_acc = train_correct / train_total
            val_acc = val_correct / val_total
            
            best_val_acc = max(best_val_acc, val_acc)
            
            # æ—©åœæª¢æŸ¥
            if epoch > 5 and val_acc < best_val_acc * 0.95:  # ç°¡å–®æ—©åœç­–ç•¥
                break
                
            scheduler.step()
        
        return {
            'accuracy': best_val_acc,
            'final_train_acc': train_acc,
            'final_val_acc': val_acc,
            'epochs_trained': epoch + 1
        }
    
    def compute_reward(self, metrics):
        # ä¸»è¦é—œæ³¨é©—è­‰æº–ç¢ºç‡ï¼Œä½†ä¹Ÿè€ƒæ…®æ•ˆç‡
        val_acc = metrics['accuracy']
        efficiency_bonus = max(0, 1.0 - metrics['epochs_trained'] / 20)  # è¨“ç·´è¶Šå¿«çå‹µè¶Šå¤š
        
        return val_acc * 0.8 + efficiency_bonus * 0.2

# å®šç¾©è¶…åƒæ•¸ç©ºé–“
cifar_space = HyperparameterSpace(
    agent0_params=[],  # é€™å€‹ä¾‹å­æ²’æœ‰å•é¡Œç‰¹å®šåƒæ•¸
    agent1_params=['conv1_filters', 'conv2_filters', 'hidden_size', 
                   'use_batch_norm', 'dropout_rate'],
    agent2_params=['batch_size', 'learning_rate', 'weight_decay'],
    bounds={
        # æ¶æ§‹åƒæ•¸
        'conv1_filters': (16, 64),
        'conv2_filters': (32, 128),
        'hidden_size': (64, 256),
        'use_batch_norm': (False, True),
        'dropout_rate': (0.0, 0.7),
        
        # è¨“ç·´åƒæ•¸
        'batch_size': (16, 128),
        'learning_rate': (1e-4, 1e-2),
        'weight_decay': (1e-5, 1e-3)
    },
    param_types={
        'conv1_filters': int,
        'conv2_filters': int,
        'hidden_size': int,
        'use_batch_norm': bool,
        'dropout_rate': float,
        'batch_size': int,
        'learning_rate': 'log_uniform',
        'weight_decay': 'log_uniform'
    }
)

# é‹è¡Œå„ªåŒ–
environment = CIFAR10Environment()
config = DefaultConfigs.standard()
config.max_steps = 50  # ç”±æ–¼æ˜¯æ¼”ç¤ºï¼Œä½¿ç”¨è¼ƒå°‘æ­¥æ•¸

optimizer = MAT_HPO_Optimizer(environment, cifar_space, config)
results = optimizer.optimize()

print("ğŸ‰ CIFAR-10 å„ªåŒ–å®Œæˆ!")
print(f"æœ€ä½³æº–ç¢ºç‡: {results['best_performance']['accuracy']:.4f}")
print(f"æœ€ä½³è¶…åƒæ•¸: {results['best_hyperparameters']}")
```

### ç¯„ä¾‹2: è‡ªç„¶èªè¨€è™•ç†å„ªåŒ–

```python
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from datasets import load_dataset
import numpy as np
from sklearn.metrics import accuracy_score, f1_score

class NLPEnvironment(BaseEnvironment):
    def __init__(self, model_name='bert-base-uncased'):
        super().__init__(name="NLP-Classification")
        self.model_name = model_name
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        
    def load_data(self):
        # ä½¿ç”¨GLUEæ•¸æ“šé›†çš„å­é›†
        dataset = load_dataset('glue', 'sst2')
        
        # ä½¿ç”¨å°å­é›†é€²è¡Œå¿«é€Ÿå¯¦é©—
        train_dataset = dataset['train'].select(range(1000))
        val_dataset = dataset['validation'].select(range(200))
        
        self.train_dataset = train_dataset
        self.val_dataset = val_dataset
        
        return {"train_size": len(train_dataset), "val_size": len(val_dataset)}
    
    def create_model(self, hyperparams):
        from transformers import AutoModelForSequenceClassification
        
        model = AutoModelForSequenceClassification.from_pretrained(
            self.model_name,
            num_labels=2,
            hidden_dropout_prob=hyperparams.get('dropout_rate', 0.1),
            attention_probs_dropout_prob=hyperparams.get('attention_dropout', 0.1)
        )
        
        # å‡çµéƒ¨åˆ†å±¤ (å¯é¸)
        if hyperparams.get('freeze_layers', 0) > 0:
            layers_to_freeze = hyperparams['freeze_layers']
            for param in model.bert.embeddings.parameters():
                param.requires_grad = False
            for layer in model.bert.encoder.layer[:layers_to_freeze]:
                for param in layer.parameters():
                    param.requires_grad = False
        
        return model
    
    def train_evaluate(self, model, hyperparams):
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        model.to(device)
        
        # æ•¸æ“šé è™•ç†
        def preprocess_data(examples):
            return self.tokenizer(examples['sentence'], 
                                 truncation=True, 
                                 padding='max_length',
                                 max_length=128)
        
        train_dataset = self.train_dataset.map(preprocess_data, batched=True)
        val_dataset = self.val_dataset.map(preprocess_data, batched=True)
        
        train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])
        val_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])
        
        # æ•¸æ“šåŠ è¼‰å™¨
        train_loader = DataLoader(train_dataset, 
                                 batch_size=hyperparams['batch_size'],
                                 shuffle=True)
        val_loader = DataLoader(val_dataset,
                               batch_size=hyperparams['batch_size'])
        
        # å„ªåŒ–å™¨ (ä½¿ç”¨ä¸åŒå­¸ç¿’ç‡ç‚ºä¸åŒå±¤)
        no_decay = ['bias', 'LayerNorm.weight']
        optimizer_grouped_parameters = [
            {
                'params': [p for n, p in model.named_parameters() 
                          if not any(nd in n for nd in no_decay)],
                'weight_decay': hyperparams.get('weight_decay', 0.01),
                'lr': hyperparams['learning_rate']
            },
            {
                'params': [p for n, p in model.named_parameters() 
                          if any(nd in n for nd in no_decay)],
                'weight_decay': 0.0,
                'lr': hyperparams['learning_rate']
            }
        ]
        
        optimizer = torch.optim.AdamW(optimizer_grouped_parameters)
        
        # å­¸ç¿’ç‡èª¿åº¦å™¨
        total_steps = len(train_loader) * hyperparams.get('epochs', 3)
        scheduler = torch.optim.lr_scheduler.LinearLR(
            optimizer,
            start_factor=1.0,
            end_factor=0.1,
            total_iters=total_steps
        )
        
        # è¨“ç·´å¾ªç’°
        model.train()
        for epoch in range(hyperparams.get('epochs', 3)):
            for batch in train_loader:
                input_ids = batch['input_ids'].to(device)
                attention_mask = batch['attention_mask'].to(device)
                labels = batch['label'].to(device)
                
                optimizer.zero_grad()
                
                outputs = model(input_ids=input_ids,
                               attention_mask=attention_mask,
                               labels=labels)
                
                loss = outputs.loss
                loss.backward()
                
                # æ¢¯åº¦è£å‰ª
                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
                
                optimizer.step()
                scheduler.step()
        
        # è©•ä¼°
        model.eval()
        predictions = []
        true_labels = []
        
        with torch.no_grad():
            for batch in val_loader:
                input_ids = batch['input_ids'].to(device)
                attention_mask = batch['attention_mask'].to(device)
                labels = batch['label']
                
                outputs = model(input_ids=input_ids,
                               attention_mask=attention_mask)
                
                preds = torch.argmax(outputs.logits, dim=-1).cpu()
                predictions.extend(preds.tolist())
                true_labels.extend(labels.tolist())
        
        # è¨ˆç®—æŒ‡æ¨™
        accuracy = accuracy_score(true_labels, predictions)
        f1 = f1_score(true_labels, predictions)
        
        return {
            'accuracy': accuracy,
            'f1': f1,
            'num_parameters': sum(p.numel() for p in model.parameters() if p.requires_grad)
        }
    
    def compute_reward(self, metrics):
        f1 = metrics['f1']
        accuracy = metrics['accuracy']
        num_params = metrics.get('num_parameters', 100000)
        
        # å¹³è¡¡æ•ˆèƒ½å’Œæ¨¡å‹å¤§å°
        efficiency_score = max(0, 1.0 - num_params / 10000000)  # åƒæ•¸è¶Šå°‘è¶Šå¥½
        
        return f1 * 0.5 + accuracy * 0.3 + efficiency_score * 0.2

# NLPè¶…åƒæ•¸ç©ºé–“
nlp_space = HyperparameterSpace(
    agent0_params=['freeze_layers'],  # é ˜åŸŸç‰¹å®šï¼šå‡çµå±¤æ•¸
    agent1_params=['dropout_rate', 'attention_dropout'],  # æ¶æ§‹åƒæ•¸
    agent2_params=['batch_size', 'learning_rate', 'weight_decay'],  # è¨“ç·´åƒæ•¸
    bounds={
        'freeze_layers': (0, 6),  # BERT baseæœ‰12å±¤ï¼Œå‡çµ0-6å±¤
        'dropout_rate': (0.0, 0.5),
        'attention_dropout': (0.0, 0.3),
        'batch_size': (8, 32),  # NLPä»»å‹™é€šå¸¸ä½¿ç”¨è¼ƒå°æ‰¹æ¬¡
        'learning_rate': (1e-5, 1e-4),  # é è¨“ç·´æ¨¡å‹éœ€è¦è¼ƒå°å­¸ç¿’ç‡
        'weight_decay': (1e-3, 1e-1)
    },
    param_types={
        'freeze_layers': int,
        'dropout_rate': float,
        'attention_dropout': float,
        'batch_size': int,
        'learning_rate': 'log_uniform',
        'weight_decay': 'log_uniform'
    }
)
```

## å¸¸è¦‹å•é¡Œ

### Q1: å¦‚ä½•é¸æ“‡åˆé©çš„ä»£ç†åˆ†å·¥ï¼Ÿ

**A:** ä»£ç†åˆ†å·¥çš„åŸå‰‡æ˜¯æŒ‰åƒæ•¸æ€§è³ªå’Œå„ªåŒ–ç­–ç•¥åˆ†çµ„ï¼š

```python
# å¥½çš„åˆ†å·¥ç¯„ä¾‹
Agent 0: å•é¡Œé ˜åŸŸåƒæ•¸ (é¡åˆ¥æ¬Šé‡ã€æ•¸æ“šå¢å¼·ã€æ­£è¦åŒ–)
Agent 1: æ¨¡å‹æ¶æ§‹åƒæ•¸ (å±¤æ•¸ã€ç¥ç¶“å…ƒæ•¸ã€æ¿€æ´»å‡½æ•¸)  
Agent 2: è¨“ç·´éç¨‹åƒæ•¸ (å­¸ç¿’ç‡ã€æ‰¹æ¬¡å¤§å°ã€å„ªåŒ–å™¨)

# é¿å…çš„åˆ†å·¥
âŒ æŒ‰å­—æ¯é †åºåˆ†é…
âŒ éš¨æ©Ÿåˆ†é…  
âŒ æ‰€æœ‰åƒæ•¸éƒ½çµ¦ä¸€å€‹ä»£ç†
```

### Q2: å¦‚ä½•è™•ç†é¡åˆ¥ä¸å¹³è¡¡çš„åƒæ•¸é¡å‹ï¼Ÿ

**A:** ä½¿ç”¨åƒæ•¸é¡å‹æ¨™è¨˜å’Œè‡ªè¨‚è™•ç†ï¼š

```python
# å°æ–¼å°æ•¸åˆ†å¸ƒçš„åƒæ•¸
'learning_rate': 'log_uniform'

# å°æ–¼é¡åˆ¥åƒæ•¸
'optimizer': ['adam', 'sgd', 'rmsprop']  # è‡ªå‹•è­˜åˆ¥ç‚ºé¡åˆ¥

# å°æ–¼æ¢ä»¶åƒæ•¸
def conditional_constraint(hyperparams):
    if hyperparams['optimizer'] == 'sgd':
        hyperparams['momentum'] = 0.9  # SGDéœ€è¦å‹•é‡
    else:
        hyperparams['momentum'] = 0.0  # å…¶ä»–å„ªåŒ–å™¨ä¸éœ€è¦
    return hyperparams
```

### Q3: è¨“ç·´éç¨‹ä¸­è¨˜æ†¶é«”ä¸è¶³æ€éº¼è¾¦ï¼Ÿ

**A:** ä½¿ç”¨æ¢¯åº¦ç´¯ç©å’Œè¨˜æ†¶é«”å„ªåŒ–ï¼š

```python
def train_evaluate(self, model, hyperparams):
    # æ¢¯åº¦ç´¯ç©
    accumulation_steps = max(1, 64 // hyperparams['batch_size'])
    
    for i, batch in enumerate(train_loader):
        outputs = model(batch)
        loss = criterion(outputs, targets) / accumulation_steps
        loss.backward()
        
        if (i + 1) % accumulation_steps == 0:
            optimizer.step()
            optimizer.zero_grad()
    
    # è¨˜æ†¶é«”æ¸…ç†
    torch.cuda.empty_cache()
```

### Q4: å¦‚ä½•è™•ç†é•·æ™‚é–“é‹è¡Œçš„å„ªåŒ–ï¼Ÿ

**A:** ä½¿ç”¨æª¢æŸ¥é»å’Œç›£æ§ï¼š

```python
# å•Ÿç”¨æª¢æŸ¥é»
environment = MyEnvironment(checkpoint_dir="./checkpoints")

# æ·»åŠ ç›£æ§å›èª¿
def monitoring_callback(env, hyperparams, metrics, reward):
    # å®šæœŸä¿å­˜
    if env.current_step % 10 == 0:
        env._save_checkpoint()
    
    # ç•°å¸¸æª¢æ¸¬
    if reward < -1.0:  # ç•°å¸¸ä½çå‹µ
        print("âš ï¸ æª¢æ¸¬åˆ°ç•°å¸¸ä½çå‹µï¼Œå¯èƒ½éœ€è¦æª¢æŸ¥")

environment.add_step_callback(monitoring_callback)
```

### Q5: å¦‚ä½•å„ªåŒ–æœç´¢æ•ˆç‡ï¼Ÿ

**A:** ä½¿ç”¨å¤šç¨®ç­–ç•¥ï¼š

```python
# 1. ä½¿ç”¨æ—©åœ
config.early_stop_patience = 20

# 2. é©æ‡‰æ€§æœç´¢ç©ºé–“
space = AdaptiveHyperparameterSpace(...)

# 3. å…ˆé©—çŸ¥è­˜åˆå§‹åŒ–
space = HyperparameterSpace(
    ...,
    default_values={
        'learning_rate': 1e-3,  # ç¶“é©—å€¼
        'batch_size': 32,
        'optimizer': 'adam'
    }
)

# 4. åˆ†éšæ®µå„ªåŒ–
# ç¬¬ä¸€éšæ®µï¼šç²—æœç´¢
config_coarse = DefaultConfigs.quick_test()
results_coarse = optimizer.optimize()

# ç¬¬äºŒéšæ®µï¼šç´°æœç´¢ (ç¸®å°æœç´¢ç¯„åœ)
refined_space = create_refined_space(results_coarse['best_hyperparameters'])
```

## æœ€ä½³å¯¦è¸

### 1. å¯¦é©—è¨­è¨ˆ

```python
# âœ… å¥½çš„å¯¦é©—è¨­è¨ˆ
class ExperimentManager:
    def __init__(self, experiment_name):
        self.experiment_name = experiment_name
        self.timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        self.base_dir = f"./experiments/{experiment_name}_{self.timestamp}"
        
    def setup_experiment(self):
        # å‰µå»ºç›®éŒ„çµæ§‹
        os.makedirs(f"{self.base_dir}/checkpoints", exist_ok=True)
        os.makedirs(f"{self.base_dir}/results", exist_ok=True)
        os.makedirs(f"{self.base_dir}/logs", exist_ok=True)
        
        # ä¿å­˜é…ç½®
        self.save_config()
        
        # è¨­ç½®æ—¥èªŒ
        self.setup_logging()
        
    def save_config(self):
        config = {
            'experiment_name': self.experiment_name,
            'timestamp': self.timestamp,
            'python_version': sys.version,
            'torch_version': torch.__version__,
            'random_seed': 42
        }
        
        with open(f"{self.base_dir}/experiment_config.json", 'w') as f:
            json.dump(config, f, indent=2)

# ä½¿ç”¨æ–¹å¼
experiment = ExperimentManager("cifar10_optimization")
experiment.setup_experiment()
```

### 2. è¶…åƒæ•¸ç©ºé–“è¨­è¨ˆ

```python
# âœ… çµæ§‹åŒ–çš„ç©ºé–“å®šç¾©
def create_comprehensive_space(problem_type='classification'):
    """æ ¹æ“šå•é¡Œé¡å‹å‰µå»ºåˆé©çš„æœç´¢ç©ºé–“"""
    
    if problem_type == 'classification':
        return HyperparameterSpace(
            agent0_params=[
                'class_weight_method',  # é¡åˆ¥å¹³è¡¡æ–¹æ³•
                'data_augmentation',    # æ•¸æ“šå¢å¼·å¼·åº¦
                'regularization_type'   # æ­£è¦åŒ–é¡å‹
            ],
            agent1_params=[
                'architecture_type',    # æ¶æ§‹é¡å‹
                'depth',               # ç¶²è·¯æ·±åº¦
                'width',               # ç¶²è·¯å¯¬åº¦
                'activation'           # æ¿€æ´»å‡½æ•¸
            ],
            agent2_params=[
                'optimizer_type',       # å„ªåŒ–å™¨
                'learning_rate',        # å­¸ç¿’ç‡
                'batch_size',          # æ‰¹æ¬¡å¤§å°
                'schedule_type'        # å­¸ç¿’ç‡èª¿åº¦
            ],
            bounds=get_classification_bounds(),
            param_types=get_classification_types()
        )
    elif problem_type == 'regression':
        return create_regression_space()
    else:
        raise ValueError(f"Unsupported problem type: {problem_type}")
```

### 3. æ•ˆèƒ½ç›£æ§

```python
# âœ… å®Œæ•´çš„ç›£æ§ç³»çµ±
class PerformanceMonitor:
    def __init__(self, log_dir):
        self.log_dir = log_dir
        self.metrics_history = []
        
    def log_step(self, step, metrics, hyperparams):
        """è¨˜éŒ„æ¯æ­¥çš„è©³ç´°ä¿¡æ¯"""
        log_entry = {
            'step': step,
            'timestamp': datetime.now().isoformat(),
            'metrics': metrics,
            'hyperparams': hyperparams,
            'system_info': self.get_system_info()
        }
        
        self.metrics_history.append(log_entry)
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        with open(f"{self.log_dir}/step_{step:04d}.json", 'w') as f:
            json.dump(log_entry, f, indent=2)
    
    def get_system_info(self):
        """ç²å–ç³»çµ±è³‡æºä¿¡æ¯"""
        import psutil
        
        return {
            'cpu_percent': psutil.cpu_percent(),
            'memory_percent': psutil.virtual_memory().percent,
            'gpu_memory': torch.cuda.memory_allocated() if torch.cuda.is_available() else 0
        }
    
    def generate_report(self):
        """ç”Ÿæˆå„ªåŒ–å ±å‘Š"""
        if not self.metrics_history:
            return
        
        # æ•ˆèƒ½è¶¨å‹¢åˆ†æ
        rewards = [entry['metrics'].get('reward', 0) for entry in self.metrics_history]
        best_idx = np.argmax(rewards)
        best_entry = self.metrics_history[best_idx]
        
        report = {
            'summary': {
                'total_steps': len(self.metrics_history),
                'best_step': best_idx,
                'best_reward': rewards[best_idx],
                'best_hyperparams': best_entry['hyperparams']
            },
            'convergence': {
                'final_reward': rewards[-1],
                'improvement': rewards[-1] - rewards[0],
                'plateau_detection': self.detect_plateau(rewards)
            }
        }
        
        with open(f"{self.log_dir}/optimization_report.json", 'w') as f:
            json.dump(report, f, indent=2)
```

### 4. å¯é‡ç¾æ€§ä¿è­‰

```python
# âœ… ç¢ºä¿å¯¦é©—å¯é‡ç¾
def ensure_reproducibility(seed=42):
    """è¨­ç½®æ‰€æœ‰éš¨æ©Ÿç¨®å­ä»¥ç¢ºä¿å¯é‡ç¾æ€§"""
    import random
    import numpy as np
    import torch
    
    # Pythonéš¨æ©Ÿç¨®å­
    random.seed(seed)
    
    # NumPyéš¨æ©Ÿç¨®å­
    np.random.seed(seed)
    
    # PyTorchéš¨æ©Ÿç¨®å­
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    
    # ç¢ºä¿CUDNNçš„ç¢ºå®šæ€§
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    
    # è¨­ç½®ç’°å¢ƒè®Šé‡
    os.environ['PYTHONHASHSEED'] = str(seed)

# åœ¨æ¯å€‹å¯¦é©—é–‹å§‹å‰èª¿ç”¨
ensure_reproducibility(42)
```

### 5. éŒ¯èª¤è™•ç†å’Œæ¢å¾©

```python
# âœ… å¥å£¯çš„éŒ¯èª¤è™•ç†
class RobustOptimizer:
    def __init__(self, environment, space, config):
        self.environment = environment
        self.space = space  
        self.config = config
        self.failed_configs = []
        
    def optimize_with_retry(self, max_retries=3):
        """å¸¶é‡è©¦æ©Ÿåˆ¶çš„å„ªåŒ–"""
        for attempt in range(max_retries):
            try:
                optimizer = MAT_HPO_Optimizer(self.environment, self.space, self.config)
                results = optimizer.optimize()
                return results
                
            except torch.cuda.OutOfMemoryError:
                print(f"GPUè¨˜æ†¶é«”ä¸è¶³ï¼Œå˜—è©¦æ¸›å°‘æ‰¹æ¬¡å¤§å° (å˜—è©¦ {attempt + 1}/{max_retries})")
                self._reduce_batch_size()
                torch.cuda.empty_cache()
                
            except Exception as e:
                print(f"å„ªåŒ–éç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}")
                if attempt == max_retries - 1:
                    raise
                
                # è¨˜éŒ„å¤±æ•—é…ç½®
                self.failed_configs.append({
                    'attempt': attempt,
                    'error': str(e),
                    'config': self.config.to_dict()
                })
                
        raise RuntimeError(f"å„ªåŒ–åœ¨ {max_retries} æ¬¡å˜—è©¦å¾Œä»ç„¶å¤±æ•—")
    
    def _reduce_batch_size(self):
        """æ¸›å°‘æ‰¹æ¬¡å¤§å°ä»¥ç¯€çœè¨˜æ†¶é«”"""
        current_batch_bounds = self.space.bounds.get('batch_size', (8, 128))
        new_max = max(8, current_batch_bounds[1] // 2)
        self.space.bounds['batch_size'] = (current_batch_bounds[0], new_max)
        print(f"å°‡æ‰¹æ¬¡å¤§å°ä¸Šé™èª¿æ•´ç‚º {new_max}")
```

---

é€™å€‹è©³ç´°ä½¿ç”¨èªªæ˜æ¶µè“‹äº†MAT-HPOå‡½å¼åº«çš„æ‰€æœ‰é‡è¦åŠŸèƒ½å’Œä½¿ç”¨å ´æ™¯ã€‚ç„¡è«–æ‚¨æ˜¯åˆå­¸è€…é‚„æ˜¯é«˜ç´šç”¨æˆ¶ï¼Œéƒ½èƒ½åœ¨é€™è£¡æ‰¾åˆ°é©åˆçš„æŒ‡å°å’Œç¯„ä¾‹ã€‚

å¦‚æœæ‚¨æœ‰ä»»ä½•å•é¡Œæˆ–éœ€è¦æ›´å…·é«”çš„å¹«åŠ©ï¼Œè«‹éš¨æ™‚è©¢å•ï¼